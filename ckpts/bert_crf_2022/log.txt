[05-04 15:53:00] INFO - ==== Train Arguments ==== {
  "output_dir": "../ckpts/bert_crf_2022",
  "overwrite_output_dir": true,
  "do_train": true,
  "do_eval": true,
  "do_predict": true,
  "evaluation_strategy": "steps",
  "prediction_loss_only": false,
  "per_device_train_batch_size": 4,
  "per_device_eval_batch_size": 16,
  "per_gpu_train_batch_size": null,
  "per_gpu_eval_batch_size": null,
  "gradient_accumulation_steps": 4,
  "eval_accumulation_steps": 500,
  "eval_delay": 0,
  "learning_rate": 3e-05,
  "weight_decay": 3e-06,
  "adam_beta1": 0.9,
  "adam_beta2": 0.999,
  "adam_epsilon": 1e-08,
  "max_grad_norm": 0.5,
  "num_train_epochs": 20.0,
  "max_steps": -1,
  "lr_scheduler_type": "cosine",
  "warmup_ratio": 0.05,
  "warmup_steps": 0,
  "log_level": -1,
  "log_level_replica": -1,
  "log_on_each_node": true,
  "logging_dir": "../ckpts/bert_crf_2022",
  "logging_strategy": "steps",
  "logging_first_step": true,
  "logging_steps": 200,
  "logging_nan_inf_filter": true,
  "save_strategy": "steps",
  "save_steps": 1000,
  "save_total_limit": 1,
  "save_on_each_node": false,
  "no_cuda": false,
  "seed": 2022,
  "data_seed": null,
  "bf16": false,
  "fp16": false,
  "fp16_opt_level": "O1",
  "half_precision_backend": "auto",
  "bf16_full_eval": false,
  "fp16_full_eval": false,
  "tf32": null,
  "local_rank": -1,
  "xpu_backend": null,
  "tpu_num_cores": null,
  "tpu_metrics_debug": false,
  "debug": [],
  "dataloader_drop_last": false,
  "eval_steps": 1000,
  "dataloader_num_workers": 8,
  "past_index": -1,
  "run_name": "../ckpts/bert_crf_2022",
  "disable_tqdm": true,
  "remove_unused_columns": true,
  "label_names": [
    "labels"
  ],
  "load_best_model_at_end": true,
  "metric_for_best_model": "f1",
  "greater_is_better": true,
  "ignore_data_skip": false,
  "sharded_ddp": [],
  "deepspeed": null,
  "label_smoothing_factor": 0.0,
  "optim": "adamw_hf",
  "adafactor": false,
  "group_by_length": false,
  "length_column_name": "length",
  "report_to": [],
  "ddp_find_unused_parameters": null,
  "ddp_bucket_cap_mb": null,
  "dataloader_pin_memory": false,
  "skip_memory_metrics": true,
  "use_legacy_prediction_loop": false,
  "push_to_hub": false,
  "resume_from_checkpoint": null,
  "hub_model_id": null,
  "hub_strategy": "every_save",
  "hub_token": "<HUB_TOKEN>",
  "gradient_checkpointing": false,
  "fp16_backend": "auto",
  "push_to_hub_model_id": null,
  "push_to_hub_organization": null,
  "push_to_hub_token": "<PUSH_TO_HUB_TOKEN>",
  "_n_gpu": 1,
  "mp_parameters": ""
}
[05-04 15:53:00] INFO - ==== Model Arguments ==== {
  "model_type": "bert",
  "head_type": "crf",
  "model_path": "../bert-base-chinese",
  "init_model": 0
}
[05-04 15:53:00] INFO - ==== Data Arguments ==== {
  "cblue_root": "../data/CBLUEDatasets",
  "max_length": 512
}
[05-04 15:53:00] INFO - ===============> Called command line: 
 run_cmeee.py --output_dir ../ckpts/bert_crf_2022 --report_to none --overwrite_output_dir true --do_train true --do_eval true --do_predict true --dataloader_pin_memory False --per_device_train_batch_size 4 --per_device_eval_batch_size 16 --gradient_accumulation_steps 4 --eval_accumulation_steps 500 --learning_rate 3e-5 --weight_decay 3e-6 --max_grad_norm 0.5 --lr_scheduler_type cosine --num_train_epochs 20 --warmup_ratio 0.05 --logging_dir ../ckpts/bert_crf_2022 --logging_strategy steps --logging_first_step true --logging_steps 200 --save_strategy steps --save_steps 1000 --evaluation_strategy steps --eval_steps 1000 --save_total_limit 1 --no_cuda false --seed 2022 --dataloader_num_workers 8 --disable_tqdm true --load_best_model_at_end true --metric_for_best_model f1 --greater_is_better true --model_type bert --model_path ../bert-base-chinese --head_type crf --cblue_root ../data/CBLUEDatasets --max_length 512 --label_names labels
[05-04 15:53:00] INFO - You can copy paste it to debug
[05-04 15:53:35] INFO - Trainset: 15000 samples
[05-04 15:53:35] INFO - Devset: 5000 samples
[05-04 15:53:38] INFO - ***** Running training *****
[05-04 15:53:38] INFO -   Num examples = 15000
[05-04 15:53:38] INFO -   Num Epochs = 20
[05-04 15:53:38] INFO -   Instantaneous batch size per device = 4
[05-04 15:53:38] INFO -   Total train batch size (w. parallel, distributed & accumulation) = 16
[05-04 15:53:38] INFO -   Gradient Accumulation steps = 4
[05-04 15:53:38] INFO -   Total optimization steps = 18740
[05-04 15:58:27] INFO - ***** Running Evaluation *****
[05-04 15:58:27] INFO -   Num examples = 5000
[05-04 15:58:27] INFO -   Batch size = 16
[05-04 16:29:00] INFO - ==== Train Arguments ==== {
  "output_dir": "../ckpts/bert_crf_2022",
  "overwrite_output_dir": true,
  "do_train": true,
  "do_eval": true,
  "do_predict": true,
  "evaluation_strategy": "steps",
  "prediction_loss_only": false,
  "per_device_train_batch_size": 4,
  "per_device_eval_batch_size": 16,
  "per_gpu_train_batch_size": null,
  "per_gpu_eval_batch_size": null,
  "gradient_accumulation_steps": 4,
  "eval_accumulation_steps": 500,
  "eval_delay": 0,
  "learning_rate": 3e-05,
  "weight_decay": 3e-06,
  "adam_beta1": 0.9,
  "adam_beta2": 0.999,
  "adam_epsilon": 1e-08,
  "max_grad_norm": 0.5,
  "num_train_epochs": 20.0,
  "max_steps": -1,
  "lr_scheduler_type": "cosine",
  "warmup_ratio": 0.05,
  "warmup_steps": 0,
  "log_level": -1,
  "log_level_replica": -1,
  "log_on_each_node": true,
  "logging_dir": "../ckpts/bert_crf_2022",
  "logging_strategy": "steps",
  "logging_first_step": true,
  "logging_steps": 200,
  "logging_nan_inf_filter": true,
  "save_strategy": "steps",
  "save_steps": 1000,
  "save_total_limit": 1,
  "save_on_each_node": false,
  "no_cuda": false,
  "seed": 2022,
  "data_seed": null,
  "bf16": false,
  "fp16": false,
  "fp16_opt_level": "O1",
  "half_precision_backend": "auto",
  "bf16_full_eval": false,
  "fp16_full_eval": false,
  "tf32": null,
  "local_rank": -1,
  "xpu_backend": null,
  "tpu_num_cores": null,
  "tpu_metrics_debug": false,
  "debug": [],
  "dataloader_drop_last": false,
  "eval_steps": 1000,
  "dataloader_num_workers": 8,
  "past_index": -1,
  "run_name": "../ckpts/bert_crf_2022",
  "disable_tqdm": true,
  "remove_unused_columns": true,
  "label_names": [
    "labels"
  ],
  "load_best_model_at_end": true,
  "metric_for_best_model": "f1",
  "greater_is_better": true,
  "ignore_data_skip": false,
  "sharded_ddp": [],
  "deepspeed": null,
  "label_smoothing_factor": 0.0,
  "optim": "adamw_hf",
  "adafactor": false,
  "group_by_length": false,
  "length_column_name": "length",
  "report_to": [],
  "ddp_find_unused_parameters": null,
  "ddp_bucket_cap_mb": null,
  "dataloader_pin_memory": false,
  "skip_memory_metrics": true,
  "use_legacy_prediction_loop": false,
  "push_to_hub": false,
  "resume_from_checkpoint": null,
  "hub_model_id": null,
  "hub_strategy": "every_save",
  "hub_token": "<HUB_TOKEN>",
  "gradient_checkpointing": false,
  "fp16_backend": "auto",
  "push_to_hub_model_id": null,
  "push_to_hub_organization": null,
  "push_to_hub_token": "<PUSH_TO_HUB_TOKEN>",
  "_n_gpu": 1,
  "mp_parameters": ""
}
[05-04 16:29:00] INFO - ==== Model Arguments ==== {
  "model_type": "bert",
  "head_type": "crf",
  "model_path": "../bert-base-chinese",
  "init_model": 0
}
[05-04 16:29:00] INFO - ==== Data Arguments ==== {
  "cblue_root": "../data/CBLUEDatasets",
  "max_length": 512
}
[05-04 16:29:00] INFO - ===============> Called command line: 
 run_cmeee.py --output_dir ../ckpts/bert_crf_2022 --report_to none --overwrite_output_dir true --do_train true --do_eval true --do_predict true --dataloader_pin_memory False --per_device_train_batch_size 4 --per_device_eval_batch_size 16 --gradient_accumulation_steps 4 --eval_accumulation_steps 500 --learning_rate 3e-5 --weight_decay 3e-6 --max_grad_norm 0.5 --lr_scheduler_type cosine --num_train_epochs 20 --warmup_ratio 0.05 --logging_dir ../ckpts/bert_crf_2022 --logging_strategy steps --logging_first_step true --logging_steps 200 --save_strategy steps --save_steps 1000 --evaluation_strategy steps --eval_steps 1000 --save_total_limit 1 --no_cuda false --seed 2022 --dataloader_num_workers 8 --disable_tqdm true --load_best_model_at_end true --metric_for_best_model f1 --greater_is_better true --model_type bert --model_path ../bert-base-chinese --head_type crf --cblue_root ../data/CBLUEDatasets --max_length 512 --label_names labels
[05-04 16:29:00] INFO - You can copy paste it to debug
[05-04 16:29:33] INFO - Trainset: 15000 samples
[05-04 16:29:33] INFO - Devset: 5000 samples
[05-04 16:29:37] INFO - ***** Running training *****
[05-04 16:29:37] INFO -   Num examples = 15000
[05-04 16:29:37] INFO -   Num Epochs = 20
[05-04 16:29:37] INFO -   Instantaneous batch size per device = 4
[05-04 16:29:37] INFO -   Total train batch size (w. parallel, distributed & accumulation) = 16
[05-04 16:29:37] INFO -   Gradient Accumulation steps = 4
[05-04 16:29:37] INFO -   Total optimization steps = 18740
[05-04 16:34:29] INFO - ***** Running Evaluation *****
[05-04 16:34:29] INFO -   Num examples = 5000
[05-04 16:34:29] INFO -   Batch size = 16
[05-04 16:41:48] INFO - ==== Train Arguments ==== {
  "output_dir": "../ckpts/bert_crf_2022",
  "overwrite_output_dir": true,
  "do_train": true,
  "do_eval": true,
  "do_predict": true,
  "evaluation_strategy": "steps",
  "prediction_loss_only": false,
  "per_device_train_batch_size": 4,
  "per_device_eval_batch_size": 16,
  "per_gpu_train_batch_size": null,
  "per_gpu_eval_batch_size": null,
  "gradient_accumulation_steps": 4,
  "eval_accumulation_steps": 500,
  "eval_delay": 0,
  "learning_rate": 3e-05,
  "weight_decay": 3e-06,
  "adam_beta1": 0.9,
  "adam_beta2": 0.999,
  "adam_epsilon": 1e-08,
  "max_grad_norm": 0.5,
  "num_train_epochs": 20.0,
  "max_steps": -1,
  "lr_scheduler_type": "cosine",
  "warmup_ratio": 0.05,
  "warmup_steps": 0,
  "log_level": -1,
  "log_level_replica": -1,
  "log_on_each_node": true,
  "logging_dir": "../ckpts/bert_crf_2022",
  "logging_strategy": "steps",
  "logging_first_step": true,
  "logging_steps": 200,
  "logging_nan_inf_filter": true,
  "save_strategy": "steps",
  "save_steps": 1000,
  "save_total_limit": 1,
  "save_on_each_node": false,
  "no_cuda": false,
  "seed": 2022,
  "data_seed": null,
  "bf16": false,
  "fp16": false,
  "fp16_opt_level": "O1",
  "half_precision_backend": "auto",
  "bf16_full_eval": false,
  "fp16_full_eval": false,
  "tf32": null,
  "local_rank": -1,
  "xpu_backend": null,
  "tpu_num_cores": null,
  "tpu_metrics_debug": false,
  "debug": [],
  "dataloader_drop_last": false,
  "eval_steps": 1000,
  "dataloader_num_workers": 8,
  "past_index": -1,
  "run_name": "../ckpts/bert_crf_2022",
  "disable_tqdm": true,
  "remove_unused_columns": true,
  "label_names": [
    "labels"
  ],
  "load_best_model_at_end": true,
  "metric_for_best_model": "f1",
  "greater_is_better": true,
  "ignore_data_skip": false,
  "sharded_ddp": [],
  "deepspeed": null,
  "label_smoothing_factor": 0.0,
  "optim": "adamw_hf",
  "adafactor": false,
  "group_by_length": false,
  "length_column_name": "length",
  "report_to": [],
  "ddp_find_unused_parameters": null,
  "ddp_bucket_cap_mb": null,
  "dataloader_pin_memory": false,
  "skip_memory_metrics": true,
  "use_legacy_prediction_loop": false,
  "push_to_hub": false,
  "resume_from_checkpoint": null,
  "hub_model_id": null,
  "hub_strategy": "every_save",
  "hub_token": "<HUB_TOKEN>",
  "gradient_checkpointing": false,
  "fp16_backend": "auto",
  "push_to_hub_model_id": null,
  "push_to_hub_organization": null,
  "push_to_hub_token": "<PUSH_TO_HUB_TOKEN>",
  "_n_gpu": 1,
  "mp_parameters": ""
}
[05-04 16:41:48] INFO - ==== Model Arguments ==== {
  "model_type": "bert",
  "head_type": "crf",
  "model_path": "../bert-base-chinese",
  "init_model": 0
}
[05-04 16:41:48] INFO - ==== Data Arguments ==== {
  "cblue_root": "../data/CBLUEDatasets",
  "max_length": 512
}
[05-04 16:41:48] INFO - ===============> Called command line: 
 run_cmeee.py --output_dir ../ckpts/bert_crf_2022 --report_to none --overwrite_output_dir true --do_train true --do_eval true --do_predict true --dataloader_pin_memory False --per_device_train_batch_size 4 --per_device_eval_batch_size 16 --gradient_accumulation_steps 4 --eval_accumulation_steps 500 --learning_rate 3e-5 --weight_decay 3e-6 --max_grad_norm 0.5 --lr_scheduler_type cosine --num_train_epochs 20 --warmup_ratio 0.05 --logging_dir ../ckpts/bert_crf_2022 --logging_strategy steps --logging_first_step true --logging_steps 200 --save_strategy steps --save_steps 1000 --evaluation_strategy steps --eval_steps 1000 --save_total_limit 1 --no_cuda false --seed 2022 --dataloader_num_workers 8 --disable_tqdm true --load_best_model_at_end true --metric_for_best_model f1 --greater_is_better true --model_type bert --model_path ../bert-base-chinese --head_type crf --cblue_root ../data/CBLUEDatasets --max_length 512 --label_names labels
[05-04 16:41:48] INFO - You can copy paste it to debug
[05-04 16:42:20] INFO - Trainset: 15000 samples
[05-04 16:42:20] INFO - Devset: 5000 samples
[05-04 16:42:24] INFO - ***** Running training *****
[05-04 16:42:24] INFO -   Num examples = 15000
[05-04 16:42:24] INFO -   Num Epochs = 20
[05-04 16:42:24] INFO -   Instantaneous batch size per device = 4
[05-04 16:42:24] INFO -   Total train batch size (w. parallel, distributed & accumulation) = 16
[05-04 16:42:24] INFO -   Gradient Accumulation steps = 4
[05-04 16:42:24] INFO -   Total optimization steps = 18740
[05-04 16:42:34] INFO - Keyboard interrupt
[05-04 16:42:37] INFO - Testset: 3000 samples
[05-04 16:42:37] INFO - ***** Running Prediction *****
[05-04 16:42:37] INFO -   Num examples = 3000
[05-04 16:42:37] INFO -   Batch size = 16
[05-04 16:43:13] INFO - ==== Train Arguments ==== {
  "output_dir": "../ckpts/bert_crf_2022",
  "overwrite_output_dir": true,
  "do_train": true,
  "do_eval": true,
  "do_predict": true,
  "evaluation_strategy": "steps",
  "prediction_loss_only": false,
  "per_device_train_batch_size": 4,
  "per_device_eval_batch_size": 16,
  "per_gpu_train_batch_size": null,
  "per_gpu_eval_batch_size": null,
  "gradient_accumulation_steps": 4,
  "eval_accumulation_steps": 500,
  "eval_delay": 0,
  "learning_rate": 3e-05,
  "weight_decay": 3e-06,
  "adam_beta1": 0.9,
  "adam_beta2": 0.999,
  "adam_epsilon": 1e-08,
  "max_grad_norm": 0.5,
  "num_train_epochs": 20.0,
  "max_steps": -1,
  "lr_scheduler_type": "cosine",
  "warmup_ratio": 0.05,
  "warmup_steps": 0,
  "log_level": -1,
  "log_level_replica": -1,
  "log_on_each_node": true,
  "logging_dir": "../ckpts/bert_crf_2022",
  "logging_strategy": "steps",
  "logging_first_step": true,
  "logging_steps": 200,
  "logging_nan_inf_filter": true,
  "save_strategy": "steps",
  "save_steps": 1000,
  "save_total_limit": 1,
  "save_on_each_node": false,
  "no_cuda": false,
  "seed": 2022,
  "data_seed": null,
  "bf16": false,
  "fp16": false,
  "fp16_opt_level": "O1",
  "half_precision_backend": "auto",
  "bf16_full_eval": false,
  "fp16_full_eval": false,
  "tf32": null,
  "local_rank": -1,
  "xpu_backend": null,
  "tpu_num_cores": null,
  "tpu_metrics_debug": false,
  "debug": [],
  "dataloader_drop_last": false,
  "eval_steps": 1000,
  "dataloader_num_workers": 8,
  "past_index": -1,
  "run_name": "../ckpts/bert_crf_2022",
  "disable_tqdm": true,
  "remove_unused_columns": true,
  "label_names": [
    "labels"
  ],
  "load_best_model_at_end": true,
  "metric_for_best_model": "f1",
  "greater_is_better": true,
  "ignore_data_skip": false,
  "sharded_ddp": [],
  "deepspeed": null,
  "label_smoothing_factor": 0.0,
  "optim": "adamw_hf",
  "adafactor": false,
  "group_by_length": false,
  "length_column_name": "length",
  "report_to": [],
  "ddp_find_unused_parameters": null,
  "ddp_bucket_cap_mb": null,
  "dataloader_pin_memory": false,
  "skip_memory_metrics": true,
  "use_legacy_prediction_loop": false,
  "push_to_hub": false,
  "resume_from_checkpoint": null,
  "hub_model_id": null,
  "hub_strategy": "every_save",
  "hub_token": "<HUB_TOKEN>",
  "gradient_checkpointing": false,
  "fp16_backend": "auto",
  "push_to_hub_model_id": null,
  "push_to_hub_organization": null,
  "push_to_hub_token": "<PUSH_TO_HUB_TOKEN>",
  "_n_gpu": 1,
  "mp_parameters": ""
}
[05-04 16:43:13] INFO - ==== Model Arguments ==== {
  "model_type": "bert",
  "head_type": "crf",
  "model_path": "../bert-base-chinese",
  "init_model": 0
}
[05-04 16:43:13] INFO - ==== Data Arguments ==== {
  "cblue_root": "../data/CBLUEDatasets",
  "max_length": 512
}
[05-04 16:43:13] INFO - ===============> Called command line: 
 run_cmeee.py --output_dir ../ckpts/bert_crf_2022 --report_to none --overwrite_output_dir true --do_train true --do_eval true --do_predict true --dataloader_pin_memory False --per_device_train_batch_size 4 --per_device_eval_batch_size 16 --gradient_accumulation_steps 4 --eval_accumulation_steps 500 --learning_rate 3e-5 --weight_decay 3e-6 --max_grad_norm 0.5 --lr_scheduler_type cosine --num_train_epochs 20 --warmup_ratio 0.05 --logging_dir ../ckpts/bert_crf_2022 --logging_strategy steps --logging_first_step true --logging_steps 200 --save_strategy steps --save_steps 1000 --evaluation_strategy steps --eval_steps 1000 --save_total_limit 1 --no_cuda false --seed 2022 --dataloader_num_workers 8 --disable_tqdm true --load_best_model_at_end true --metric_for_best_model f1 --greater_is_better true --model_type bert --model_path ../bert-base-chinese --head_type crf --cblue_root ../data/CBLUEDatasets --max_length 512 --label_names labels
[05-04 16:43:13] INFO - You can copy paste it to debug
[05-04 16:43:46] INFO - Trainset: 15000 samples
[05-04 16:43:46] INFO - Devset: 5000 samples
[05-04 16:43:49] INFO - ***** Running training *****
[05-04 16:43:49] INFO -   Num examples = 15000
[05-04 16:43:49] INFO -   Num Epochs = 20
[05-04 16:43:49] INFO -   Instantaneous batch size per device = 4
[05-04 16:43:49] INFO -   Total train batch size (w. parallel, distributed & accumulation) = 16
[05-04 16:43:49] INFO -   Gradient Accumulation steps = 4
[05-04 16:43:49] INFO -   Total optimization steps = 18740
[05-04 16:44:54] INFO - Keyboard interrupt
[05-04 16:44:57] INFO - Testset: 3000 samples
[05-04 16:44:57] INFO - ***** Running Prediction *****
[05-04 16:44:57] INFO -   Num examples = 3000
[05-04 16:44:57] INFO -   Batch size = 16
[05-04 16:55:06] INFO - ==== Train Arguments ==== {
  "output_dir": "../ckpts/bert_crf_2022",
  "overwrite_output_dir": true,
  "do_train": true,
  "do_eval": true,
  "do_predict": true,
  "evaluation_strategy": "steps",
  "prediction_loss_only": false,
  "per_device_train_batch_size": 4,
  "per_device_eval_batch_size": 16,
  "per_gpu_train_batch_size": null,
  "per_gpu_eval_batch_size": null,
  "gradient_accumulation_steps": 4,
  "eval_accumulation_steps": 500,
  "eval_delay": 0,
  "learning_rate": 3e-05,
  "weight_decay": 3e-06,
  "adam_beta1": 0.9,
  "adam_beta2": 0.999,
  "adam_epsilon": 1e-08,
  "max_grad_norm": 0.5,
  "num_train_epochs": 20.0,
  "max_steps": -1,
  "lr_scheduler_type": "cosine",
  "warmup_ratio": 0.05,
  "warmup_steps": 0,
  "log_level": -1,
  "log_level_replica": -1,
  "log_on_each_node": true,
  "logging_dir": "../ckpts/bert_crf_2022",
  "logging_strategy": "steps",
  "logging_first_step": true,
  "logging_steps": 200,
  "logging_nan_inf_filter": true,
  "save_strategy": "steps",
  "save_steps": 1000,
  "save_total_limit": 1,
  "save_on_each_node": false,
  "no_cuda": false,
  "seed": 2022,
  "data_seed": null,
  "bf16": false,
  "fp16": false,
  "fp16_opt_level": "O1",
  "half_precision_backend": "auto",
  "bf16_full_eval": false,
  "fp16_full_eval": false,
  "tf32": null,
  "local_rank": -1,
  "xpu_backend": null,
  "tpu_num_cores": null,
  "tpu_metrics_debug": false,
  "debug": [],
  "dataloader_drop_last": false,
  "eval_steps": 1000,
  "dataloader_num_workers": 8,
  "past_index": -1,
  "run_name": "../ckpts/bert_crf_2022",
  "disable_tqdm": true,
  "remove_unused_columns": true,
  "label_names": [
    "labels"
  ],
  "load_best_model_at_end": true,
  "metric_for_best_model": "f1",
  "greater_is_better": true,
  "ignore_data_skip": false,
  "sharded_ddp": [],
  "deepspeed": null,
  "label_smoothing_factor": 0.0,
  "optim": "adamw_hf",
  "adafactor": false,
  "group_by_length": false,
  "length_column_name": "length",
  "report_to": [],
  "ddp_find_unused_parameters": null,
  "ddp_bucket_cap_mb": null,
  "dataloader_pin_memory": false,
  "skip_memory_metrics": true,
  "use_legacy_prediction_loop": false,
  "push_to_hub": false,
  "resume_from_checkpoint": null,
  "hub_model_id": null,
  "hub_strategy": "every_save",
  "hub_token": "<HUB_TOKEN>",
  "gradient_checkpointing": false,
  "fp16_backend": "auto",
  "push_to_hub_model_id": null,
  "push_to_hub_organization": null,
  "push_to_hub_token": "<PUSH_TO_HUB_TOKEN>",
  "_n_gpu": 1,
  "mp_parameters": ""
}
[05-04 16:55:06] INFO - ==== Model Arguments ==== {
  "model_type": "bert",
  "head_type": "crf",
  "model_path": "../bert-base-chinese",
  "init_model": 0
}
[05-04 16:55:06] INFO - ==== Data Arguments ==== {
  "cblue_root": "../data/CBLUEDatasets",
  "max_length": 512
}
[05-04 16:55:06] INFO - ===============> Called command line: 
 run_cmeee.py --output_dir ../ckpts/bert_crf_2022 --report_to none --overwrite_output_dir true --do_train true --do_eval true --do_predict true --dataloader_pin_memory False --per_device_train_batch_size 4 --per_device_eval_batch_size 16 --gradient_accumulation_steps 4 --eval_accumulation_steps 500 --learning_rate 3e-5 --weight_decay 3e-6 --max_grad_norm 0.5 --lr_scheduler_type cosine --num_train_epochs 20 --warmup_ratio 0.05 --logging_dir ../ckpts/bert_crf_2022 --logging_strategy steps --logging_first_step true --logging_steps 200 --save_strategy steps --save_steps 1000 --evaluation_strategy steps --eval_steps 1000 --save_total_limit 1 --no_cuda false --seed 2022 --dataloader_num_workers 8 --disable_tqdm true --load_best_model_at_end true --metric_for_best_model f1 --greater_is_better true --model_type bert --model_path ../bert-base-chinese --head_type crf --cblue_root ../data/CBLUEDatasets --max_length 512 --label_names labels
[05-04 16:55:06] INFO - You can copy paste it to debug
[05-04 16:55:38] INFO - Trainset: 15000 samples
[05-04 16:55:38] INFO - Devset: 5000 samples
[05-04 16:55:42] INFO - ***** Running training *****
[05-04 16:55:42] INFO -   Num examples = 15000
[05-04 16:55:42] INFO -   Num Epochs = 20
[05-04 16:55:42] INFO -   Instantaneous batch size per device = 4
[05-04 16:55:42] INFO -   Total train batch size (w. parallel, distributed & accumulation) = 16
[05-04 16:55:42] INFO -   Gradient Accumulation steps = 4
[05-04 16:55:42] INFO -   Total optimization steps = 18740
[05-04 16:56:48] INFO - Keyboard interrupt
[05-04 16:56:51] INFO - Testset: 3000 samples
[05-04 16:56:51] INFO - ***** Running Prediction *****
[05-04 16:56:51] INFO -   Num examples = 3000
[05-04 16:56:51] INFO -   Batch size = 16
[05-04 16:57:33] INFO - ==== Train Arguments ==== {
  "output_dir": "../ckpts/bert_crf_2022",
  "overwrite_output_dir": true,
  "do_train": true,
  "do_eval": true,
  "do_predict": true,
  "evaluation_strategy": "steps",
  "prediction_loss_only": false,
  "per_device_train_batch_size": 4,
  "per_device_eval_batch_size": 16,
  "per_gpu_train_batch_size": null,
  "per_gpu_eval_batch_size": null,
  "gradient_accumulation_steps": 4,
  "eval_accumulation_steps": 500,
  "eval_delay": 0,
  "learning_rate": 3e-05,
  "weight_decay": 3e-06,
  "adam_beta1": 0.9,
  "adam_beta2": 0.999,
  "adam_epsilon": 1e-08,
  "max_grad_norm": 0.5,
  "num_train_epochs": 20.0,
  "max_steps": -1,
  "lr_scheduler_type": "cosine",
  "warmup_ratio": 0.05,
  "warmup_steps": 0,
  "log_level": -1,
  "log_level_replica": -1,
  "log_on_each_node": true,
  "logging_dir": "../ckpts/bert_crf_2022",
  "logging_strategy": "steps",
  "logging_first_step": true,
  "logging_steps": 200,
  "logging_nan_inf_filter": true,
  "save_strategy": "steps",
  "save_steps": 1000,
  "save_total_limit": 1,
  "save_on_each_node": false,
  "no_cuda": false,
  "seed": 2022,
  "data_seed": null,
  "bf16": false,
  "fp16": false,
  "fp16_opt_level": "O1",
  "half_precision_backend": "auto",
  "bf16_full_eval": false,
  "fp16_full_eval": false,
  "tf32": null,
  "local_rank": -1,
  "xpu_backend": null,
  "tpu_num_cores": null,
  "tpu_metrics_debug": false,
  "debug": [],
  "dataloader_drop_last": false,
  "eval_steps": 1000,
  "dataloader_num_workers": 8,
  "past_index": -1,
  "run_name": "../ckpts/bert_crf_2022",
  "disable_tqdm": true,
  "remove_unused_columns": true,
  "label_names": [
    "labels"
  ],
  "load_best_model_at_end": true,
  "metric_for_best_model": "f1",
  "greater_is_better": true,
  "ignore_data_skip": false,
  "sharded_ddp": [],
  "deepspeed": null,
  "label_smoothing_factor": 0.0,
  "optim": "adamw_hf",
  "adafactor": false,
  "group_by_length": false,
  "length_column_name": "length",
  "report_to": [],
  "ddp_find_unused_parameters": null,
  "ddp_bucket_cap_mb": null,
  "dataloader_pin_memory": false,
  "skip_memory_metrics": true,
  "use_legacy_prediction_loop": false,
  "push_to_hub": false,
  "resume_from_checkpoint": null,
  "hub_model_id": null,
  "hub_strategy": "every_save",
  "hub_token": "<HUB_TOKEN>",
  "gradient_checkpointing": false,
  "fp16_backend": "auto",
  "push_to_hub_model_id": null,
  "push_to_hub_organization": null,
  "push_to_hub_token": "<PUSH_TO_HUB_TOKEN>",
  "_n_gpu": 1,
  "mp_parameters": ""
}
[05-04 16:57:33] INFO - ==== Model Arguments ==== {
  "model_type": "bert",
  "head_type": "crf",
  "model_path": "../bert-base-chinese",
  "init_model": 0
}
[05-04 16:57:33] INFO - ==== Data Arguments ==== {
  "cblue_root": "../data/CBLUEDatasets",
  "max_length": 512
}
[05-04 16:57:33] INFO - ===============> Called command line: 
 run_cmeee.py --output_dir ../ckpts/bert_crf_2022 --report_to none --overwrite_output_dir true --do_train true --do_eval true --do_predict true --dataloader_pin_memory False --per_device_train_batch_size 4 --per_device_eval_batch_size 16 --gradient_accumulation_steps 4 --eval_accumulation_steps 500 --learning_rate 3e-5 --weight_decay 3e-6 --max_grad_norm 0.5 --lr_scheduler_type cosine --num_train_epochs 20 --warmup_ratio 0.05 --logging_dir ../ckpts/bert_crf_2022 --logging_strategy steps --logging_first_step true --logging_steps 200 --save_strategy steps --save_steps 1000 --evaluation_strategy steps --eval_steps 1000 --save_total_limit 1 --no_cuda false --seed 2022 --dataloader_num_workers 8 --disable_tqdm true --load_best_model_at_end true --metric_for_best_model f1 --greater_is_better true --model_type bert --model_path ../bert-base-chinese --head_type crf --cblue_root ../data/CBLUEDatasets --max_length 512 --label_names labels
[05-04 16:57:33] INFO - You can copy paste it to debug
[05-04 16:58:05] INFO - Trainset: 15000 samples
[05-04 16:58:05] INFO - Devset: 5000 samples
[05-04 16:58:09] INFO - ***** Running training *****
[05-04 16:58:09] INFO -   Num examples = 15000
[05-04 16:58:09] INFO -   Num Epochs = 20
[05-04 16:58:09] INFO -   Instantaneous batch size per device = 4
[05-04 16:58:09] INFO -   Total train batch size (w. parallel, distributed & accumulation) = 16
[05-04 16:58:09] INFO -   Gradient Accumulation steps = 4
[05-04 16:58:09] INFO -   Total optimization steps = 18740
[05-04 16:59:13] INFO - Keyboard interrupt
[05-04 16:59:16] INFO - Testset: 3000 samples
[05-04 16:59:16] INFO - ***** Running Prediction *****
[05-04 16:59:16] INFO -   Num examples = 3000
[05-04 16:59:16] INFO -   Batch size = 16
[05-04 16:59:22] INFO - `CMeEE_test.json` saved
[05-04 16:59:43] INFO - ==== Train Arguments ==== {
  "output_dir": "../ckpts/bert_crf_2022",
  "overwrite_output_dir": true,
  "do_train": true,
  "do_eval": true,
  "do_predict": true,
  "evaluation_strategy": "steps",
  "prediction_loss_only": false,
  "per_device_train_batch_size": 4,
  "per_device_eval_batch_size": 16,
  "per_gpu_train_batch_size": null,
  "per_gpu_eval_batch_size": null,
  "gradient_accumulation_steps": 4,
  "eval_accumulation_steps": 500,
  "eval_delay": 0,
  "learning_rate": 3e-05,
  "weight_decay": 3e-06,
  "adam_beta1": 0.9,
  "adam_beta2": 0.999,
  "adam_epsilon": 1e-08,
  "max_grad_norm": 0.5,
  "num_train_epochs": 20.0,
  "max_steps": -1,
  "lr_scheduler_type": "cosine",
  "warmup_ratio": 0.05,
  "warmup_steps": 0,
  "log_level": -1,
  "log_level_replica": -1,
  "log_on_each_node": true,
  "logging_dir": "../ckpts/bert_crf_2022",
  "logging_strategy": "steps",
  "logging_first_step": true,
  "logging_steps": 200,
  "logging_nan_inf_filter": true,
  "save_strategy": "steps",
  "save_steps": 1000,
  "save_total_limit": 1,
  "save_on_each_node": false,
  "no_cuda": false,
  "seed": 2022,
  "data_seed": null,
  "bf16": false,
  "fp16": false,
  "fp16_opt_level": "O1",
  "half_precision_backend": "auto",
  "bf16_full_eval": false,
  "fp16_full_eval": false,
  "tf32": null,
  "local_rank": -1,
  "xpu_backend": null,
  "tpu_num_cores": null,
  "tpu_metrics_debug": false,
  "debug": [],
  "dataloader_drop_last": false,
  "eval_steps": 1000,
  "dataloader_num_workers": 8,
  "past_index": -1,
  "run_name": "../ckpts/bert_crf_2022",
  "disable_tqdm": true,
  "remove_unused_columns": true,
  "label_names": [
    "labels"
  ],
  "load_best_model_at_end": true,
  "metric_for_best_model": "f1",
  "greater_is_better": true,
  "ignore_data_skip": false,
  "sharded_ddp": [],
  "deepspeed": null,
  "label_smoothing_factor": 0.0,
  "optim": "adamw_hf",
  "adafactor": false,
  "group_by_length": false,
  "length_column_name": "length",
  "report_to": [],
  "ddp_find_unused_parameters": null,
  "ddp_bucket_cap_mb": null,
  "dataloader_pin_memory": false,
  "skip_memory_metrics": true,
  "use_legacy_prediction_loop": false,
  "push_to_hub": false,
  "resume_from_checkpoint": null,
  "hub_model_id": null,
  "hub_strategy": "every_save",
  "hub_token": "<HUB_TOKEN>",
  "gradient_checkpointing": false,
  "fp16_backend": "auto",
  "push_to_hub_model_id": null,
  "push_to_hub_organization": null,
  "push_to_hub_token": "<PUSH_TO_HUB_TOKEN>",
  "_n_gpu": 1,
  "mp_parameters": ""
}
[05-04 16:59:43] INFO - ==== Model Arguments ==== {
  "model_type": "bert",
  "head_type": "crf",
  "model_path": "../bert-base-chinese",
  "init_model": 0
}
[05-04 16:59:43] INFO - ==== Data Arguments ==== {
  "cblue_root": "../data/CBLUEDatasets",
  "max_length": 512
}
[05-04 16:59:43] INFO - ===============> Called command line: 
 run_cmeee.py --output_dir ../ckpts/bert_crf_2022 --report_to none --overwrite_output_dir true --do_train true --do_eval true --do_predict true --dataloader_pin_memory False --per_device_train_batch_size 4 --per_device_eval_batch_size 16 --gradient_accumulation_steps 4 --eval_accumulation_steps 500 --learning_rate 3e-5 --weight_decay 3e-6 --max_grad_norm 0.5 --lr_scheduler_type cosine --num_train_epochs 20 --warmup_ratio 0.05 --logging_dir ../ckpts/bert_crf_2022 --logging_strategy steps --logging_first_step true --logging_steps 200 --save_strategy steps --save_steps 1000 --evaluation_strategy steps --eval_steps 1000 --save_total_limit 1 --no_cuda false --seed 2022 --dataloader_num_workers 8 --disable_tqdm true --load_best_model_at_end true --metric_for_best_model f1 --greater_is_better true --model_type bert --model_path ../bert-base-chinese --head_type crf --cblue_root ../data/CBLUEDatasets --max_length 512 --label_names labels
[05-04 16:59:43] INFO - You can copy paste it to debug
[05-04 17:00:15] INFO - Trainset: 15000 samples
[05-04 17:00:15] INFO - Devset: 5000 samples
[05-04 17:00:18] INFO - ***** Running training *****
[05-04 17:00:18] INFO -   Num examples = 15000
[05-04 17:00:18] INFO -   Num Epochs = 20
[05-04 17:00:18] INFO -   Instantaneous batch size per device = 4
[05-04 17:00:18] INFO -   Total train batch size (w. parallel, distributed & accumulation) = 16
[05-04 17:00:18] INFO -   Gradient Accumulation steps = 4
[05-04 17:00:18] INFO -   Total optimization steps = 18740
[05-04 17:05:07] INFO - ***** Running Evaluation *****
[05-04 17:05:07] INFO -   Num examples = 5000
[05-04 17:05:07] INFO -   Batch size = 16
[05-04 17:05:30] INFO - Saving model checkpoint to ../ckpts/bert_crf_2022/checkpoint-1000
[05-04 17:10:14] INFO - ***** Running Evaluation *****
[05-04 17:10:14] INFO -   Num examples = 5000
[05-04 17:10:14] INFO -   Batch size = 16
[05-04 17:10:38] INFO - Saving model checkpoint to ../ckpts/bert_crf_2022/checkpoint-2000
[05-04 17:10:40] INFO - Deleting older checkpoint [../ckpts/bert_crf_2022/checkpoint-1000] due to args.save_total_limit
[05-04 17:15:25] INFO - ***** Running Evaluation *****
[05-04 17:15:25] INFO -   Num examples = 5000
[05-04 17:15:25] INFO -   Batch size = 16
[05-04 17:15:48] INFO - Saving model checkpoint to ../ckpts/bert_crf_2022/checkpoint-3000
[05-04 17:15:50] INFO - Deleting older checkpoint [../ckpts/bert_crf_2022/checkpoint-2000] due to args.save_total_limit
[05-04 17:20:30] INFO - ***** Running Evaluation *****
[05-04 17:20:30] INFO -   Num examples = 5000
[05-04 17:20:30] INFO -   Batch size = 16
[05-04 17:20:54] INFO - Saving model checkpoint to ../ckpts/bert_crf_2022/checkpoint-4000
[05-04 17:20:56] INFO - Deleting older checkpoint [../ckpts/bert_crf_2022/checkpoint-3000] due to args.save_total_limit
[05-04 17:25:39] INFO - ***** Running Evaluation *****
[05-04 17:25:39] INFO -   Num examples = 5000
[05-04 17:25:39] INFO -   Batch size = 16
[05-04 17:26:02] INFO - Saving model checkpoint to ../ckpts/bert_crf_2022/checkpoint-5000
[05-04 17:30:46] INFO - ***** Running Evaluation *****
[05-04 17:30:46] INFO -   Num examples = 5000
[05-04 17:30:46] INFO -   Batch size = 16
[05-04 17:31:10] INFO - Saving model checkpoint to ../ckpts/bert_crf_2022/checkpoint-6000
[05-04 17:31:11] INFO - Deleting older checkpoint [../ckpts/bert_crf_2022/checkpoint-5000] due to args.save_total_limit
[05-04 17:35:55] INFO - ***** Running Evaluation *****
[05-04 17:35:55] INFO -   Num examples = 5000
[05-04 17:35:55] INFO -   Batch size = 16
[05-04 17:36:19] INFO - Saving model checkpoint to ../ckpts/bert_crf_2022/checkpoint-7000
[05-04 17:36:21] INFO - Deleting older checkpoint [../ckpts/bert_crf_2022/checkpoint-6000] due to args.save_total_limit
[05-04 17:41:03] INFO - ***** Running Evaluation *****
[05-04 17:41:03] INFO -   Num examples = 5000
[05-04 17:41:03] INFO -   Batch size = 16
[05-04 17:41:27] INFO - Saving model checkpoint to ../ckpts/bert_crf_2022/checkpoint-8000
[05-04 17:41:28] INFO - Deleting older checkpoint [../ckpts/bert_crf_2022/checkpoint-7000] due to args.save_total_limit
[05-04 17:46:12] INFO - ***** Running Evaluation *****
[05-04 17:46:12] INFO -   Num examples = 5000
[05-04 17:46:12] INFO -   Batch size = 16
[05-04 17:46:35] INFO - Saving model checkpoint to ../ckpts/bert_crf_2022/checkpoint-9000
[05-04 17:46:37] INFO - Deleting older checkpoint [../ckpts/bert_crf_2022/checkpoint-8000] due to args.save_total_limit
[05-04 17:51:19] INFO - ***** Running Evaluation *****
[05-04 17:51:19] INFO -   Num examples = 5000
[05-04 17:51:19] INFO -   Batch size = 16
[05-04 17:51:43] INFO - Saving model checkpoint to ../ckpts/bert_crf_2022/checkpoint-10000
[05-04 17:51:44] INFO - Deleting older checkpoint [../ckpts/bert_crf_2022/checkpoint-9000] due to args.save_total_limit
[05-04 17:56:28] INFO - ***** Running Evaluation *****
[05-04 17:56:28] INFO -   Num examples = 5000
[05-04 17:56:28] INFO -   Batch size = 16
[05-04 17:56:52] INFO - Saving model checkpoint to ../ckpts/bert_crf_2022/checkpoint-11000
[05-04 17:56:54] INFO - Deleting older checkpoint [../ckpts/bert_crf_2022/checkpoint-10000] due to args.save_total_limit
[05-04 18:01:36] INFO - ***** Running Evaluation *****
[05-04 18:01:36] INFO -   Num examples = 5000
[05-04 18:01:36] INFO -   Batch size = 16
[05-04 18:02:00] INFO - Saving model checkpoint to ../ckpts/bert_crf_2022/checkpoint-12000
[05-04 18:02:02] INFO - Deleting older checkpoint [../ckpts/bert_crf_2022/checkpoint-11000] due to args.save_total_limit
[05-04 18:06:46] INFO - ***** Running Evaluation *****
[05-04 18:06:46] INFO -   Num examples = 5000
[05-04 18:06:46] INFO -   Batch size = 16
[05-04 18:07:10] INFO - Saving model checkpoint to ../ckpts/bert_crf_2022/checkpoint-13000
[05-04 18:07:18] INFO - Deleting older checkpoint [../ckpts/bert_crf_2022/checkpoint-12000] due to args.save_total_limit
[05-04 18:11:59] INFO - ***** Running Evaluation *****
[05-04 18:11:59] INFO -   Num examples = 5000
[05-04 18:11:59] INFO -   Batch size = 16
[05-04 18:12:23] INFO - Saving model checkpoint to ../ckpts/bert_crf_2022/checkpoint-14000
[05-04 18:12:25] INFO - Deleting older checkpoint [../ckpts/bert_crf_2022/checkpoint-13000] due to args.save_total_limit
[05-04 18:17:14] INFO - ***** Running Evaluation *****
[05-04 18:17:14] INFO -   Num examples = 5000
[05-04 18:17:14] INFO -   Batch size = 16
[05-04 18:17:38] INFO - Saving model checkpoint to ../ckpts/bert_crf_2022/checkpoint-15000
[05-04 18:17:39] INFO - Deleting older checkpoint [../ckpts/bert_crf_2022/checkpoint-14000] due to args.save_total_limit
[05-04 18:22:21] INFO - ***** Running Evaluation *****
[05-04 18:22:21] INFO -   Num examples = 5000
[05-04 18:22:21] INFO -   Batch size = 16
[05-04 18:22:45] INFO - Saving model checkpoint to ../ckpts/bert_crf_2022/checkpoint-16000
[05-04 18:22:47] INFO - Deleting older checkpoint [../ckpts/bert_crf_2022/checkpoint-15000] due to args.save_total_limit
[05-04 18:27:31] INFO - ***** Running Evaluation *****
[05-04 18:27:31] INFO -   Num examples = 5000
[05-04 18:27:31] INFO -   Batch size = 16
[05-04 18:27:55] INFO - Saving model checkpoint to ../ckpts/bert_crf_2022/checkpoint-17000
[05-04 18:27:56] INFO - Deleting older checkpoint [../ckpts/bert_crf_2022/checkpoint-4000] due to args.save_total_limit
[05-04 18:27:56] INFO - Deleting older checkpoint [../ckpts/bert_crf_2022/checkpoint-16000] due to args.save_total_limit
[05-04 18:32:37] INFO - ***** Running Evaluation *****
[05-04 18:32:37] INFO -   Num examples = 5000
[05-04 18:32:37] INFO -   Batch size = 16
[05-04 18:33:01] INFO - Saving model checkpoint to ../ckpts/bert_crf_2022/checkpoint-18000
[05-04 18:36:28] INFO - 

Training completed. Do not forget to share your model on huggingface.co/models =)


[05-04 18:36:28] INFO - Loading best model from ../ckpts/bert_crf_2022/checkpoint-17000 (score: 0.6241680903689727).
[05-04 18:36:32] INFO - Testset: 3000 samples
[05-04 18:36:32] INFO - ***** Running Prediction *****
[05-04 18:36:32] INFO -   Num examples = 3000
[05-04 18:36:32] INFO -   Batch size = 16
[05-04 18:36:37] INFO - `CMeEE_test.json` saved
