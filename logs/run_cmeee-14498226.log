/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
Some weights of the model checkpoint at trueto/medbert-base-chinese were not used when initializing BertForCRFHeadNestedNER: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias']
- This IS expected if you are initializing BertForCRFHeadNestedNER from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForCRFHeadNestedNER from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForCRFHeadNestedNER were not initialized from the model checkpoint at trueto/medbert-base-chinese and are newly initialized: ['classifier2.crf.transitions', 'classifier1.crf.start_transitions', 'classifier2.crf.start_transitions', 'classifier1.crf.transitions', 'classifier2.linear.bias', 'classifier1.crf.end_transitions', 'classifier2.crf.end_transitions', 'classifier1.linear.weight', 'classifier1.linear.bias', 'classifier2.linear.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Traceback (most recent call last):
  File "run_cmeee.py", line 155, in <module>
    main()
  File "run_cmeee.py", line 128, in main
    trainer = Trainer(
  File "/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/transformers/trainer.py", line 374, in __init__
    self._move_model_to_device(model, args.device)
  File "/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/transformers/trainer.py", line 522, in _move_model_to_device
    model = model.to(device)
  File "/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 907, in to
    return self._apply(convert)
  File "/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 578, in _apply
    module._apply(fn)
  File "/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 601, in _apply
    param_applied = fn(param)
  File "/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py", line 905, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
