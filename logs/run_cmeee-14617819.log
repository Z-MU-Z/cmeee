You are using a model of type roberta to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
Some weights of the model checkpoint at clue/roberta_chinese_large were not used when initializing BertForCRFHeadNestedNER: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForCRFHeadNestedNER from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForCRFHeadNestedNER from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForCRFHeadNestedNER were not initialized from the model checkpoint at clue/roberta_chinese_large and are newly initialized: ['classifier1.crf.end_transitions', 'classifier1.crf.transitions', 'classifier2.linear.weight', 'classifier1.linear.bias', 'classifier2.linear.bias', 'classifier2.crf.start_transitions', 'classifier2.crf.transitions', 'classifier1.linear.weight', 'classifier2.crf.end_transitions', 'classifier1.crf.start_transitions']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. 
The class this function is called from is 'BertTokenizer'.
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
***** Running training *****
  Num examples = 15000
  Num Epochs = 20
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 4
  Total optimization steps = 18740
{'loss': 446.8569, 'learning_rate': 3.2017075773746e-08, 'epoch': 0.0}
{'loss': 212.257, 'learning_rate': 6.4034151547492e-06, 'epoch': 0.21}
{'loss': 70.8504, 'learning_rate': 1.28068303094984e-05, 'epoch': 0.43}
{'loss': 60.4424, 'learning_rate': 1.92102454642476e-05, 'epoch': 0.64}
{'loss': 61.4145, 'learning_rate': 2.56136606189968e-05, 'epoch': 0.85}
***** Running Evaluation *****
  Num examples = 5000
  Batch size = 16
{'loss': 59.5365, 'learning_rate': 2.999907306079514e-05, 'epoch': 1.07}
Saving model checkpoint to ../ckpts/roberta_large_crf_nested_2022/checkpoint-1000
Configuration saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-1000/config.json
Model weights saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-1000/pytorch_model.bin
tokenizer config file saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-1000/tokenizer_config.json
Special tokens file saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-1000/special_tokens_map.json
{'eval_loss': 250.10220336914062, 'eval_f1': 0.5975471698113207, 'eval_runtime': 39.7266, 'eval_samples_per_second': 125.86, 'eval_steps_per_second': 7.879, 'epoch': 1.07}
{'loss': 49.5391, 'learning_rate': 2.9983848674539465e-05, 'epoch': 1.28}
{'loss': 51.6609, 'learning_rate': 2.9949962569027466e-05, 'epoch': 1.49}
{'loss': 50.7942, 'learning_rate': 2.9897456947901306e-05, 'epoch': 1.71}
{'loss': 53.2888, 'learning_rate': 2.9826397204584404e-05, 'epoch': 1.92}
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
***** Running Evaluation *****
  Num examples = 5000
  Batch size = 16
{'loss': 42.9098, 'learning_rate': 2.9736871840836827e-05, 'epoch': 2.13}
Saving model checkpoint to ../ckpts/roberta_large_crf_nested_2022/checkpoint-2000
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
Configuration saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-2000/config.json
Model weights saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-2000/pytorch_model.bin
tokenizer config file saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-2000/tokenizer_config.json
Special tokens file saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-2000/special_tokens_map.json
Deleting older checkpoint [../ckpts/roberta_large_crf_nested_2022/checkpoint-1000] due to args.save_total_limit
{'eval_loss': 245.19735717773438, 'eval_f1': 0.6125699745547074, 'eval_runtime': 39.4182, 'eval_samples_per_second': 126.845, 'eval_steps_per_second': 7.94, 'epoch': 2.13}
{'loss': 37.7866, 'learning_rate': 2.962899235653026e-05, 'epoch': 2.35}
{'loss': 39.6585, 'learning_rate': 2.9502893110779867e-05, 'epoch': 2.56}
{'loss': 37.9993, 'learning_rate': 2.9358731154605965e-05, 'epoch': 2.77}
{'loss': 40.2997, 'learning_rate': 2.9196686035333938e-05, 'epoch': 2.99}
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
***** Running Evaluation *****
  Num examples = 5000
  Batch size = 16
{'loss': 28.2599, 'learning_rate': 2.901695957297602e-05, 'epoch': 3.2}
Saving model checkpoint to ../ckpts/roberta_large_crf_nested_2022/checkpoint-3000
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
Configuration saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-3000/config.json
Model weights saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-3000/pytorch_model.bin
tokenizer config file saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-3000/tokenizer_config.json
Special tokens file saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-3000/special_tokens_map.json
Deleting older checkpoint [../ckpts/roberta_large_crf_nested_2022/checkpoint-2000] due to args.save_total_limit
{'eval_loss': 268.12640380859375, 'eval_f1': 0.620243757431629, 'eval_runtime': 39.3263, 'eval_samples_per_second': 127.141, 'eval_steps_per_second': 7.959, 'epoch': 3.2}
{'loss': 28.0629, 'learning_rate': 2.8819775608873405e-05, 'epoch': 3.41}
{'loss': 29.7932, 'learning_rate': 2.8605379726911757e-05, 'epoch': 3.63}
{'loss': 27.807, 'learning_rate': 2.837403894765742e-05, 'epoch': 3.84}
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
***** Running Evaluation *****
  Num examples = 5000
  Batch size = 16
{'loss': 26.4604, 'learning_rate': 2.8126041395795084e-05, 'epoch': 4.06}
{'loss': 20.6648, 'learning_rate': 2.7861695941281318e-05, 'epoch': 4.27}
Saving model checkpoint to ../ckpts/roberta_large_crf_nested_2022/checkpoint-4000
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
Configuration saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-4000/config.json
Model weights saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-4000/pytorch_model.bin
tokenizer config file saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-4000/tokenizer_config.json
Special tokens file saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-4000/special_tokens_map.json
Deleting older checkpoint [../ckpts/roberta_large_crf_nested_2022/checkpoint-3000] due to args.save_total_limit
{'eval_loss': 297.6112365722656, 'eval_f1': 0.6438203876456088, 'eval_runtime': 39.4813, 'eval_samples_per_second': 126.642, 'eval_steps_per_second': 7.928, 'epoch': 4.27}
{'loss': 21.4282, 'learning_rate': 2.7581331814660674e-05, 'epoch': 4.48}
{'loss': 21.1309, 'learning_rate': 2.7285298197023694e-05, 'epoch': 4.7}
{'loss': 21.2651, 'learning_rate': 2.69739637851173e-05, 'epoch': 4.91}
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
***** Running Evaluation *****
  Num examples = 5000
  Batch size = 16
{'loss': 17.8016, 'learning_rate': 2.6647716332149374e-05, 'epoch': 5.12}
{'loss': 15.3269, 'learning_rate': 2.6306962164859286e-05, 'epoch': 5.34}
Saving model checkpoint to ../ckpts/roberta_large_crf_nested_2022/checkpoint-5000
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
Configuration saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-5000/config.json
Model weights saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-5000/pytorch_model.bin
tokenizer config file saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-5000/tokenizer_config.json
Special tokens file saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-5000/special_tokens_map.json
{'eval_loss': 317.2737121582031, 'eval_f1': 0.6264680105170902, 'eval_runtime': 39.4071, 'eval_samples_per_second': 126.881, 'eval_steps_per_second': 7.943, 'epoch': 5.34}
{'loss': 14.7488, 'learning_rate': 2.5952125677456013e-05, 'epoch': 5.55}
{'loss': 16.3961, 'learning_rate': 2.5583648803053904e-05, 'epoch': 5.76}
{'loss': 16.7783, 'learning_rate': 2.5201990463264616e-05, 'epoch': 5.98}
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
***** Running Evaluation *****
  Num examples = 5000
  Batch size = 16
{'loss': 11.1902, 'learning_rate': 2.4807625996630578e-05, 'epoch': 6.19}
{'loss': 12.4639, 'learning_rate': 2.4401046566611924e-05, 'epoch': 6.4}
Saving model checkpoint to ../ckpts/roberta_large_crf_nested_2022/checkpoint-6000
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
Configuration saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-6000/config.json
Model weights saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-6000/pytorch_model.bin
tokenizer config file saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-6000/tokenizer_config.json
Special tokens file saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-6000/special_tokens_map.json
Deleting older checkpoint [../ckpts/roberta_large_crf_nested_2022/checkpoint-5000] due to args.save_total_limit
{'eval_loss': 339.6258544921875, 'eval_f1': 0.6271729629989318, 'eval_runtime': 39.4468, 'eval_samples_per_second': 126.753, 'eval_steps_per_second': 7.935, 'epoch': 6.4}
{'loss': 11.405, 'learning_rate': 2.398275854986416e-05, 'epoch': 6.62}
{'loss': 12.8129, 'learning_rate': 2.355328290556848e-05, 'epoch': 6.83}
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
***** Running Evaluation *****
  Num examples = 5000
  Batch size = 16
{'loss': 11.1852, 'learning_rate': 2.311315452660025e-05, 'epoch': 7.04}
{'loss': 8.3785, 'learning_rate': 2.266292157334358e-05, 'epoch': 7.26}
{'loss': 8.7823, 'learning_rate': 2.2203144790981905e-05, 'epoch': 7.47}
Saving model checkpoint to ../ckpts/roberta_large_crf_nested_2022/checkpoint-7000
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
Configuration saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-7000/config.json
Model weights saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-7000/pytorch_model.bin
tokenizer config file saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-7000/tokenizer_config.json
Special tokens file saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-7000/special_tokens_map.json
Deleting older checkpoint [../ckpts/roberta_large_crf_nested_2022/checkpoint-6000] due to args.save_total_limit
{'eval_loss': 374.835693359375, 'eval_f1': 0.6226868060418126, 'eval_runtime': 39.5039, 'eval_samples_per_second': 126.57, 'eval_steps_per_second': 7.923, 'epoch': 7.47}
{'loss': 9.0089, 'learning_rate': 2.173439681111472e-05, 'epoch': 7.68}
{'loss': 9.7078, 'learning_rate': 2.1257261438570296e-05, 'epoch': 7.9}
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
***** Running Evaluation *****
  Num examples = 5000
  Batch size = 16
{'loss': 8.2306, 'learning_rate': 2.0772332924302645e-05, 'epoch': 8.11}
{'loss': 6.6802, 'learning_rate': 2.0280215225278316e-05, 'epoch': 8.32}
{'loss': 6.9754, 'learning_rate': 1.9781521252274758e-05, 'epoch': 8.54}
Saving model checkpoint to ../ckpts/roberta_large_crf_nested_2022/checkpoint-8000
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
Configuration saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-8000/config.json
Model weights saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-8000/pytorch_model.bin
tokenizer config file saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-8000/tokenizer_config.json
Special tokens file saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-8000/special_tokens_map.json
Deleting older checkpoint [../ckpts/roberta_large_crf_nested_2022/checkpoint-7000] due to args.save_total_limit
{'eval_loss': 396.3726501464844, 'eval_f1': 0.6310809171418174, 'eval_runtime': 39.5205, 'eval_samples_per_second': 126.516, 'eval_steps_per_second': 7.92, 'epoch': 8.54}
{'loss': 6.8629, 'learning_rate': 1.9276872106527107e-05, 'epoch': 8.75}
{'loss': 6.4421, 'learning_rate': 1.8766896306174174e-05, 'epoch': 8.96}
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
***** Running Evaluation *****
  Num examples = 5000
  Batch size = 16
{'loss': 5.4771, 'learning_rate': 1.8252229003466906e-05, 'epoch': 9.18}
{'loss': 5.1112, 'learning_rate': 1.7733511193714465e-05, 'epoch': 9.39}
{'loss': 4.8799, 'learning_rate': 1.721138891695298e-05, 'epoch': 9.6}
Saving model checkpoint to ../ckpts/roberta_large_crf_nested_2022/checkpoint-9000
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
Configuration saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-9000/config.json
Model weights saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-9000/pytorch_model.bin
tokenizer config file saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-9000/tokenizer_config.json
Special tokens file saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-9000/special_tokens_map.json
Deleting older checkpoint [../ckpts/roberta_large_crf_nested_2022/checkpoint-8000] due to args.save_total_limit
{'eval_loss': 439.14111328125, 'eval_f1': 0.6269217628971643, 'eval_runtime': 39.3922, 'eval_samples_per_second': 126.929, 'eval_steps_per_second': 7.946, 'epoch': 9.6}
{'loss': 5.226, 'learning_rate': 1.6686512453331278e-05, 'epoch': 9.82}
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
***** Running Evaluation *****
  Num examples = 5000
  Batch size = 16
{'loss': 5.0, 'learning_rate': 1.615953551321586e-05, 'epoch': 10.03}
{'loss': 3.5791, 'learning_rate': 1.5631114423023553e-05, 'epoch': 10.25}
{'loss': 3.5211, 'learning_rate': 1.51019073077961e-05, 'epoch': 10.46}
{'loss': 3.8333, 'learning_rate': 1.4572573271534564e-05, 'epoch': 10.67}
Saving model checkpoint to ../ckpts/roberta_large_crf_nested_2022/checkpoint-10000
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
Configuration saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-10000/config.json
Model weights saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-10000/pytorch_model.bin
tokenizer config file saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-10000/tokenizer_config.json
Special tokens file saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-10000/special_tokens_map.json
Deleting older checkpoint [../ckpts/roberta_large_crf_nested_2022/checkpoint-9000] due to args.save_total_limit
{'eval_loss': 456.89910888671875, 'eval_f1': 0.6350968179725177, 'eval_runtime': 39.4503, 'eval_samples_per_second': 126.742, 'eval_steps_per_second': 7.934, 'epoch': 10.67}
{'loss': 3.96, 'learning_rate': 1.4043771576314543e-05, 'epoch': 10.89}
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
***** Running Evaluation *****
  Num examples = 5000
  Batch size = 16
{'loss': 3.3321, 'learning_rate': 1.3516160821204461e-05, 'epoch': 11.1}
{'loss': 2.9061, 'learning_rate': 1.2990398122009627e-05, 'epoch': 11.31}
{'loss': 2.8971, 'learning_rate': 1.2467138292863648e-05, 'epoch': 11.53}
{'loss': 3.0052, 'learning_rate': 1.194703303068643e-05, 'epoch': 11.74}
Saving model checkpoint to ../ckpts/roberta_large_crf_nested_2022/checkpoint-11000
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
Configuration saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-11000/config.json
Model weights saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-11000/pytorch_model.bin
tokenizer config file saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-11000/tokenizer_config.json
Special tokens file saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-11000/special_tokens_map.json
Deleting older checkpoint [../ckpts/roberta_large_crf_nested_2022/checkpoint-10000] due to args.save_total_limit
{'eval_loss': 502.5309143066406, 'eval_f1': 0.6305262128325508, 'eval_runtime': 39.2522, 'eval_samples_per_second': 127.381, 'eval_steps_per_second': 7.974, 'epoch': 11.74}
{'loss': 2.5476, 'learning_rate': 1.1430730103524588e-05, 'epoch': 11.95}
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
***** Running Evaluation *****
  Num examples = 5000
  Batch size = 16
{'loss': 2.1499, 'learning_rate': 1.0918872543785011e-05, 'epoch': 12.17}
{'loss': 2.308, 'learning_rate': 1.0412097847366568e-05, 'epoch': 12.38}
{'loss': 2.1042, 'learning_rate': 9.91103717968718e-06, 'epoch': 12.59}
{'loss': 1.627, 'learning_rate': 9.416314589595287e-06, 'epoch': 12.81}
Saving model checkpoint to ../ckpts/roberta_large_crf_nested_2022/checkpoint-12000
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
Configuration saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-12000/config.json
Model weights saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-12000/pytorch_model.bin
tokenizer config file saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-12000/tokenizer_config.json
Special tokens file saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-12000/special_tokens_map.json
Deleting older checkpoint [../ckpts/roberta_large_crf_nested_2022/checkpoint-11000] due to args.save_total_limit
{'eval_loss': 572.4003295898438, 'eval_f1': 0.637942309089588, 'eval_runtime': 39.554, 'eval_samples_per_second': 126.41, 'eval_steps_per_second': 7.913, 'epoch': 12.81}
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
***** Running Evaluation *****
  Num examples = 5000
  Batch size = 16
{'loss': 2.1246, 'learning_rate': 8.928546232144682e-06, 'epoch': 13.02}
{'loss': 1.5058, 'learning_rate': 8.448339601200703e-06, 'epoch': 13.23}
{'loss': 1.7004, 'learning_rate': 7.976292772833542e-06, 'epoch': 13.45}
{'loss': 1.9435, 'learning_rate': 7.5129936604410325e-06, 'epoch': 13.66}
{'loss': 1.5154, 'learning_rate': 7.05901928252857e-06, 'epoch': 13.87}
Saving model checkpoint to ../ckpts/roberta_large_crf_nested_2022/checkpoint-13000
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
Configuration saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-13000/config.json
Model weights saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-13000/pytorch_model.bin
tokenizer config file saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-13000/tokenizer_config.json
Special tokens file saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-13000/special_tokens_map.json
Deleting older checkpoint [../ckpts/roberta_large_crf_nested_2022/checkpoint-12000] due to args.save_total_limit
{'eval_loss': 616.7252197265625, 'eval_f1': 0.6342373785113551, 'eval_runtime': 39.492, 'eval_samples_per_second': 126.608, 'eval_steps_per_second': 7.926, 'epoch': 13.87}
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
***** Running Evaluation *****
  Num examples = 5000
  Batch size = 16
{'loss': 1.1, 'learning_rate': 6.6149350440581205e-06, 'epoch': 14.09}
{'loss': 1.0453, 'learning_rate': 6.181294032261449e-06, 'epoch': 14.3}
{'loss': 1.0941, 'learning_rate': 5.758636327794474e-06, 'epoch': 14.51}
{'loss': 1.6101, 'learning_rate': 5.347488332090737e-06, 'epoch': 14.73}
{'loss': 1.3216, 'learning_rate': 4.948362111751759e-06, 'epoch': 14.94}
Saving model checkpoint to ../ckpts/roberta_large_crf_nested_2022/checkpoint-14000
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
Configuration saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-14000/config.json
Model weights saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-14000/pytorch_model.bin
tokenizer config file saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-14000/tokenizer_config.json
Special tokens file saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-14000/special_tokens_map.json
Deleting older checkpoint [../ckpts/roberta_large_crf_nested_2022/checkpoint-13000] due to args.save_total_limit
{'eval_loss': 642.4937133789062, 'eval_f1': 0.6376069202145653, 'eval_runtime': 39.3537, 'eval_samples_per_second': 127.053, 'eval_steps_per_second': 7.953, 'epoch': 14.94}
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
{'loss': 1.1015, 'learning_rate': 4.561754760790814e-06, 'epoch': 15.15}
{'loss': 0.885, 'learning_rate': 4.188147781524345e-06, 'epoch': 15.37}
{'loss': 1.0801, 'learning_rate': 3.82800648488219e-06, 'epoch': 15.58}
{'loss': 0.8081, 'learning_rate': 3.4817794108834383e-06, 'epoch': 15.79}
***** Running Evaluation *****
  Num examples = 5000
  Batch size = 16
{'loss': 0.8548, 'learning_rate': 3.149897769999731e-06, 'epoch': 16.01}
Saving model checkpoint to ../ckpts/roberta_large_crf_nested_2022/checkpoint-15000
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
Configuration saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-15000/config.json
Model weights saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-15000/pytorch_model.bin
tokenizer config file saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-15000/tokenizer_config.json
Special tokens file saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-15000/special_tokens_map.json
Deleting older checkpoint [../ckpts/roberta_large_crf_nested_2022/checkpoint-14000] due to args.save_total_limit
{'eval_loss': 680.8262939453125, 'eval_f1': 0.6386587197751611, 'eval_runtime': 39.4396, 'eval_samples_per_second': 126.776, 'eval_steps_per_second': 7.936, 'epoch': 16.01}
{'loss': 0.6556, 'learning_rate': 2.832774906101727e-06, 'epoch': 16.22}
{'loss': 0.7168, 'learning_rate': 2.5308057816576373e-06, 'epoch': 16.44}
{'loss': 0.737, 'learning_rate': 2.24436648582499e-06, 'epoch': 16.65}
{'loss': 0.6837, 'learning_rate': 1.9738137660482375e-06, 'epoch': 16.86}
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
***** Running Evaluation *****
  Num examples = 5000
  Batch size = 16
{'loss': 0.7108, 'learning_rate': 1.7194845837456474e-06, 'epoch': 17.08}
Saving model checkpoint to ../ckpts/roberta_large_crf_nested_2022/checkpoint-16000
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
Configuration saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-16000/config.json
Model weights saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-16000/pytorch_model.bin
tokenizer config file saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-16000/tokenizer_config.json
Special tokens file saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-16000/special_tokens_map.json
Deleting older checkpoint [../ckpts/roberta_large_crf_nested_2022/checkpoint-15000] due to args.save_total_limit
{'eval_loss': 735.8870239257812, 'eval_f1': 0.6422938370168435, 'eval_runtime': 39.5109, 'eval_samples_per_second': 126.547, 'eval_steps_per_second': 7.922, 'epoch': 17.08}
{'loss': 0.5067, 'learning_rate': 1.4816956946387988e-06, 'epoch': 17.29}
{'loss': 0.5415, 'learning_rate': 1.2607432542473563e-06, 'epoch': 17.5}
{'loss': 0.7032, 'learning_rate': 1.0569024490405016e-06, 'epoch': 17.72}
{'loss': 0.5206, 'learning_rate': 8.704271537044001e-07, 'epoch': 17.93}
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
***** Running Evaluation *****
  Num examples = 5000
  Batch size = 16
{'loss': 0.6173, 'learning_rate': 7.015496149525225e-07, 'epoch': 18.14}
Saving model checkpoint to ../ckpts/roberta_large_crf_nested_2022/checkpoint-17000
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
Configuration saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-17000/config.json
Model weights saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-17000/pytorch_model.bin
tokenizer config file saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-17000/tokenizer_config.json
Special tokens file saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-17000/special_tokens_map.json
Deleting older checkpoint [../ckpts/roberta_large_crf_nested_2022/checkpoint-16000] due to args.save_total_limit
{'eval_loss': 746.72509765625, 'eval_f1': 0.6412597208134087, 'eval_runtime': 39.4772, 'eval_samples_per_second': 126.655, 'eval_steps_per_second': 7.929, 'epoch': 18.14}
{'loss': 0.4011, 'learning_rate': 5.504801622726635e-07, 'epoch': 18.36}
{'loss': 0.5651, 'learning_rate': 4.1740694597088183e-07, 'epoch': 18.57}
{'loss': 0.4333, 'learning_rate': 3.024957028386516e-07, 'epoch': 18.78}
{'loss': 0.6216, 'learning_rate': 2.0588954973501163e-07, 'epoch': 19.0}
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
***** Running Evaluation *****
  Num examples = 5000
  Batch size = 16
{'loss': 0.3553, 'learning_rate': 1.27708805340892e-07, 'epoch': 19.21}
Saving model checkpoint to ../ckpts/roberta_large_crf_nested_2022/checkpoint-18000
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/transformers/configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.
  warnings.warn(
Configuration saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-18000/config.json
Model weights saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-18000/pytorch_model.bin
tokenizer config file saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-18000/tokenizer_config.json
Special tokens file saved in ../ckpts/roberta_large_crf_nested_2022/checkpoint-18000/special_tokens_map.json
Deleting older checkpoint [../ckpts/roberta_large_crf_nested_2022/checkpoint-17000] due to args.save_total_limit


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from ../ckpts/roberta_large_crf_nested_2022/checkpoint-4000 (score: 0.6438203876456088).
{'eval_loss': 758.8090209960938, 'eval_f1': 0.6432604604024902, 'eval_runtime': 39.4629, 'eval_samples_per_second': 126.701, 'eval_steps_per_second': 7.932, 'epoch': 19.21}
{'loss': 0.3616, 'learning_rate': 6.805084030750142e-08, 'epoch': 19.42}
{'loss': 0.4854, 'learning_rate': 2.6989955985518567e-08, 'epoch': 19.64}
{'loss': 0.624, 'learning_rate': 4.577291886044277e-09, 'epoch': 19.85}
{'train_runtime': 11099.5064, 'train_samples_per_second': 27.028, 'train_steps_per_second': 1.688, 'train_loss': 14.934985557564294, 'epoch': 20.0}
Traceback (most recent call last):
  File "run_cmeee.py", line 155, in <module>
    main()
  File "run_cmeee.py", line 143, in main
    print(s_wte.learned_embedding)
UnboundLocalError: local variable 's_wte' referenced before assignment
