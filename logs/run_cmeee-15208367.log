Some weights of the model checkpoint at ../bert-base-chinese were not used when initializing BertAdapterForLinearHeadNER: ['bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'cls.predictions.decoder.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.embeddings.position_embeddings.weight', 'bert.pooler.dense.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.embeddings.word_embeddings.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.10.attention.self.value.bias', 'cls.predictions.transform.dense.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.pooler.dense.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.1.intermediate.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.4.intermediate.dense.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertAdapterForLinearHeadNER from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertAdapterForLinearHeadNER from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertAdapterForLinearHeadNER were not initialized from the model checkpoint at ../bert-base-chinese and are newly initialized: ['bert.bert.encoder.layer.7.attention.self.value.bias', 'bert.bert.encoder.layer.10.attention.self.query.bias', 'bert.bert.encoder.layer.9.output.LayerNorm.weight', 'bert.bert.encoder.layer.10.output.LayerNorm.bias', 'bert.bert.encoder.layer.7.intermediate.dense.bias', 'bert.bert.encoder.layer.0.output.dense.weight', 'bert.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.bert.encoder.layer.8.output.dense.bias', 'bert.bert.encoder.layer.3.attention.self.value.bias', 'bert.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.bert.embeddings.LayerNorm.bias', 'bert.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.bert.encoder.layer.9.output.LayerNorm.bias', 'bert.bert.encoder.layer.4.attention.self.key.bias', 'bert.bert.encoder.layer.10.attention.output.dense.bias', 'bert.bert.encoder.layer.4.attention.output.dense.bias', 'bert.bert.encoder.layer.3.attention.self.key.bias', 'bert.bert.encoder.layer.7.output.LayerNorm.weight', 'bert.bert.encoder.layer.5.output.dense.bias', 'bert.bert.encoder.layer.4.attention.self.query.bias', 'bert.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.bert.encoder.layer.5.attention.output.dense.bias', 'bert.bert.encoder.layer.6.attention.self.query.weight', 'bert.bert.encoder.layer.4.attention.output.dense.weight', 'bert.bert.encoder.layer.7.output.LayerNorm.bias', 'bert.bert.encoder.layer.8.intermediate.dense.weight', 'bert.bert.encoder.layer.10.output.LayerNorm.weight', 'bert.bert.encoder.layer.6.output.LayerNorm.bias', 'bert.bert.encoder.layer.0.output.LayerNorm.weight', 'bert.bert.encoder.layer.11.output.dense.bias', 'bert.bert.encoder.layer.11.attention.self.value.bias', 'bert.bert.encoder.layer.2.output.LayerNorm.weight', 'bert.bert.encoder.layer.0.attention.self.key.bias', 'bert.bert.encoder.layer.2.attention.self.key.bias', 'bert.bert.encoder.layer.2.attention.self.key.weight', 'bert.bert.encoder.layer.3.attention.output.dense.bias', 'bert.bert.encoder.layer.10.attention.self.key.weight', 'bert.bert.encoder.layer.7.attention.self.query.bias', 'bert.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.bert.encoder.layer.8.output.dense.weight', 'bert.bert.encoder.layer.8.attention.self.key.bias', 'bert.bert.encoder.layer.10.attention.self.value.bias', 'bert.bert.encoder.layer.7.intermediate.dense.weight', 'bert.bert.encoder.layer.0.intermediate.dense.bias', 'bert.bert.encoder.layer.10.intermediate.dense.weight', 'bert.bert.encoder.layer.2.attention.output.dense.weight', 'bert.bert.encoder.layer.8.attention.self.value.bias', 'bert.bert.encoder.layer.0.intermediate.dense.weight', 'bert.bert.encoder.layer.3.attention.output.dense.weight', 'bert.bert.encoder.layer.6.attention.self.value.weight', 'bert.bert.encoder.layer.10.output.dense.bias', 'bert.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.bert.encoder.layer.0.attention.self.value.weight', 'bert.bert.encoder.layer.3.attention.self.query.weight', 'bert.bert.embeddings.word_embeddings.weight', 'bert.bert.encoder.layer.11.intermediate.dense.weight', 'bert.bert.encoder.layer.1.attention.self.key.weight', 'bert.bert.encoder.layer.2.attention.self.value.weight', 'bert.bert.encoder.layer.10.attention.self.key.bias', 'bert.bert.encoder.layer.10.attention.self.value.weight', 'bert.bert.encoder.layer.10.intermediate.dense.bias', 'bert.bert.encoder.layer.6.attention.self.value.bias', 'bert.bert.encoder.layer.3.attention.self.query.bias', 'bert.bert.encoder.layer.8.attention.self.query.bias', 'bert.bert.encoder.layer.8.attention.self.key.weight', 'bert.bert.encoder.layer.1.attention.self.value.bias', 'bert.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.bert.encoder.layer.9.attention.self.value.weight', 'bert.bert.encoder.layer.1.attention.output.dense.weight', 'bert.bert.encoder.layer.1.intermediate.dense.bias', 'bert.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.bert.encoder.layer.1.intermediate.dense.weight', 'bert.bert.encoder.layer.8.attention.self.value.weight', 'bert.bert.encoder.layer.0.output.dense.bias', 'bert.bert.encoder.layer.1.attention.self.value.weight', 'bert.bert.encoder.layer.11.attention.self.value.weight', 'classifier.layers.1.weight', 'bert.bert.encoder.layer.9.intermediate.dense.bias', 'bert.bert.encoder.layer.2.attention.output.dense.bias', 'bert.bert.encoder.layer.7.attention.output.dense.bias', 'bert.bert.encoder.layer.8.output.LayerNorm.weight', 'bert.bert.encoder.layer.7.output.dense.bias', 'bert.bert.encoder.layer.2.attention.self.query.weight', 'bert.bert.encoder.layer.9.output.dense.bias', 'bert.bert.encoder.layer.0.attention.self.query.bias', 'bert.bert.encoder.layer.0.attention.output.dense.bias', 'bert.bert.encoder.layer.5.attention.output.dense.weight', 'bert.bert.encoder.layer.4.output.LayerNorm.bias', 'bert.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.bert.encoder.layer.6.attention.output.dense.weight', 'bert.bert.encoder.layer.8.output.LayerNorm.bias', 'bert.bert.encoder.layer.2.intermediate.dense.bias', 'bert.bert.encoder.layer.6.attention.self.key.bias', 'bert.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.bert.encoder.layer.3.intermediate.dense.bias', 'bert.bert.encoder.layer.1.attention.self.key.bias', 'bert.bert.encoder.layer.3.attention.self.value.weight', 'bert.bert.encoder.layer.9.attention.self.query.weight', 'bert.bert.encoder.layer.9.output.dense.weight', 'bert.bert.embeddings.position_embeddings.weight', 'bert.bert.encoder.layer.0.attention.self.query.weight', 'bert.bert.encoder.layer.9.attention.self.key.weight', 'bert.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.bert.encoder.layer.1.attention.self.query.weight', 'bert.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.bert.encoder.layer.3.intermediate.dense.weight', 'bert.bert.encoder.layer.11.output.LayerNorm.bias', 'bert.bert.encoder.layer.6.output.dense.bias', 'bert.bert.encoder.layer.0.output.LayerNorm.bias', 'bert.bert.encoder.layer.4.output.dense.bias', 'bert.bert.encoder.layer.8.attention.self.query.weight', 'bert.bert.pooler.dense.weight', 'bert.bert.encoder.layer.5.attention.self.value.bias', 'bert.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.bert.encoder.layer.0.attention.self.key.weight', 'classifier.layers.1.bias', 'bert.bert.encoder.layer.5.attention.self.query.bias', 'bert.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.bert.encoder.layer.10.attention.output.dense.weight', 'bert.bert.encoder.layer.5.attention.self.query.weight', 'bert.bert.encoder.layer.6.attention.self.query.bias', 'bert.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.bert.encoder.layer.2.attention.self.query.bias', 'bert.bert.encoder.layer.2.output.LayerNorm.bias', 'bert.bert.encoder.layer.6.attention.self.key.weight', 'bert.bert.encoder.layer.9.intermediate.dense.weight', 'bert.bert.encoder.layer.8.attention.output.dense.bias', 'bert.bert.encoder.layer.11.output.dense.weight', 'bert.bert.encoder.layer.3.output.dense.bias', 'bert.bert.encoder.layer.3.output.LayerNorm.weight', 'bert.bert.encoder.layer.5.output.dense.weight', 'bert.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.bert.encoder.layer.1.output.dense.weight', 'bert.bert.encoder.layer.1.output.dense.bias', 'bert.bert.encoder.layer.11.attention.self.key.bias', 'bert.bert.encoder.layer.5.output.LayerNorm.bias', 'bert.bert.encoder.layer.4.attention.self.value.bias', 'bert.bert.encoder.layer.7.attention.self.value.weight', 'bert.bert.encoder.layer.5.intermediate.dense.bias', 'bert.bert.encoder.layer.9.attention.self.query.bias', 'bert.bert.encoder.layer.9.attention.output.dense.bias', 'bert.bert.encoder.layer.11.attention.self.query.bias', 'bert.bert.encoder.layer.5.attention.self.key.weight', 'bert.bert.encoder.layer.7.attention.self.query.weight', 'bert.bert.encoder.layer.1.output.LayerNorm.bias', 'bert.bert.encoder.layer.4.attention.self.query.weight', 'bert.bert.encoder.layer.3.attention.self.key.weight', 'bert.bert.encoder.layer.4.attention.self.key.weight', 'bert.bert.encoder.layer.4.output.dense.weight', 'bert.bert.encoder.layer.8.attention.output.dense.weight', 'bert.bert.encoder.layer.4.output.LayerNorm.weight', 'bert.bert.encoder.layer.9.attention.self.key.bias', 'bert.bert.encoder.layer.5.attention.self.key.bias', 'bert.bert.encoder.layer.11.attention.output.dense.weight', 'bert.bert.encoder.layer.6.attention.output.dense.bias', 'bert.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.bert.embeddings.LayerNorm.weight', 'bert.bert.encoder.layer.6.intermediate.dense.weight', 'bert.bert.encoder.layer.0.attention.output.dense.weight', 'bert.bert.encoder.layer.1.attention.self.query.bias', 'bert.bert.encoder.layer.2.output.dense.bias', 'bert.bert.embeddings.token_type_embeddings.weight', 'bert.bert.encoder.layer.4.intermediate.dense.bias', 'bert.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.bert.encoder.layer.4.attention.self.value.weight', 'bert.bert.encoder.layer.4.intermediate.dense.weight', 'bert.bert.encoder.layer.7.attention.output.dense.weight', 'bert.bert.encoder.layer.0.attention.self.value.bias', 'bert.bert.encoder.layer.3.output.dense.weight', 'bert.bert.encoder.layer.9.attention.output.dense.weight', 'bert.bert.encoder.layer.11.intermediate.dense.bias', 'bert.bert.encoder.layer.6.output.dense.weight', 'bert.bert.encoder.layer.6.intermediate.dense.bias', 'bert.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.bert.encoder.layer.6.output.LayerNorm.weight', 'bert.bert.encoder.layer.10.output.dense.weight', 'bert.bert.encoder.layer.1.output.LayerNorm.weight', 'bert.bert.encoder.layer.5.output.LayerNorm.weight', 'bert.bert.encoder.layer.7.attention.self.key.bias', 'bert.bert.encoder.layer.3.output.LayerNorm.bias', 'bert.bert.encoder.layer.2.intermediate.dense.weight', 'bert.bert.encoder.layer.10.attention.self.query.weight', 'bert.bert.encoder.layer.5.attention.self.value.weight', 'bert.bert.encoder.layer.2.output.dense.weight', 'bert.bert.encoder.layer.11.attention.output.dense.bias', 'bert.bert.pooler.dense.bias', 'bert.bert.encoder.layer.7.attention.self.key.weight', 'bert.bert.encoder.layer.11.attention.self.key.weight', 'bert.bert.encoder.layer.1.attention.output.dense.bias', 'bert.bert.encoder.layer.11.output.LayerNorm.weight', 'bert.bert.encoder.layer.8.intermediate.dense.bias', 'bert.bert.encoder.layer.9.attention.self.value.bias', 'bert.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.bert.encoder.layer.11.attention.self.query.weight', 'bert.bert.encoder.layer.2.attention.self.value.bias', 'bert.bert.encoder.layer.5.intermediate.dense.weight', 'bert.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.bert.encoder.layer.7.output.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 30000
  Num Epochs = 20
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 4
  Total optimization steps = 37500
use_adapter
***** Running Evaluation *****
  Num examples = 5000
  Batch size = 16
{'loss': 2.8901, 'learning_rate': 1.6e-08, 'epoch': 0.0}
{'loss': 2.6843, 'learning_rate': 3.2000000000000003e-06, 'epoch': 0.11}
{'loss': 1.826, 'learning_rate': 6.4000000000000006e-06, 'epoch': 0.21}
{'loss': 1.4444, 'learning_rate': 9.600000000000001e-06, 'epoch': 0.32}
{'loss': 1.0912, 'learning_rate': 1.2800000000000001e-05, 'epoch': 0.43}
{'loss': 0.9362, 'learning_rate': 1.6e-05, 'epoch': 0.53}
Saving model checkpoint to ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-1000
Configuration saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-1000/config.json
Model weights saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-1000/pytorch_model.bin
tokenizer config file saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-1000/tokenizer_config.json
Special tokens file saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-1000/special_tokens_map.json
{'eval_loss': 0.6032838821411133, 'eval_f1': 0.0, 'eval_runtime': 8.5002, 'eval_samples_per_second': 588.223, 'eval_steps_per_second': 36.823, 'epoch': 0.53}
{'loss': 0.9094, 'learning_rate': 1.9200000000000003e-05, 'epoch': 0.64}
{'loss': 0.8776, 'learning_rate': 2.2400000000000002e-05, 'epoch': 0.75}
{'loss': 0.8749, 'learning_rate': 2.5600000000000002e-05, 'epoch': 0.85}
{'loss': 0.8488, 'learning_rate': 2.88e-05, 'epoch': 0.96}
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
***** Running Evaluation *****
  Num examples = 5000
  Batch size = 16
{'loss': 0.8383, 'learning_rate': 2.9999088688415145e-05, 'epoch': 1.07}
Saving model checkpoint to ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-2000
Configuration saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-2000/config.json
Model weights saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-2000/pytorch_model.bin
tokenizer config file saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-2000/tokenizer_config.json
Special tokens file saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-2000/special_tokens_map.json
Deleting older checkpoint [../ckpts/bert_adapter_adapter_linear_2022/checkpoint-1000] due to args.save_total_limit
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
***** Running Evaluation *****
  Num examples = 5000
  Batch size = 16
{'eval_loss': 0.540596067905426, 'eval_f1': 0.0033620294432275483, 'eval_runtime': 8.2159, 'eval_samples_per_second': 608.574, 'eval_steps_per_second': 38.097, 'epoch': 1.07}
{'loss': 0.821, 'learning_rate': 2.9993839892985038e-05, 'epoch': 1.17}
{'loss': 0.8138, 'learning_rate': 2.9983927172082925e-05, 'epoch': 1.28}
{'loss': 0.8061, 'learning_rate': 2.996935360912118e-05, 'epoch': 1.39}
{'loss': 0.7939, 'learning_rate': 2.995012373729557e-05, 'epoch': 1.49}
{'loss': 0.7894, 'learning_rate': 2.9926243538175172e-05, 'epoch': 1.6}
Saving model checkpoint to ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-3000
Configuration saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-3000/config.json
Model weights saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-3000/pytorch_model.bin
tokenizer config file saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-3000/tokenizer_config.json
Special tokens file saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-3000/special_tokens_map.json
Deleting older checkpoint [../ckpts/bert_adapter_adapter_linear_2022/checkpoint-2000] due to args.save_total_limit
{'eval_loss': 0.5149797201156616, 'eval_f1': 0.016747754491017963, 'eval_runtime': 8.2758, 'eval_samples_per_second': 604.168, 'eval_steps_per_second': 37.821, 'epoch': 1.6}
{'loss': 0.792, 'learning_rate': 2.9897720439841768e-05, 'epoch': 1.71}
{'loss': 0.7844, 'learning_rate': 2.98645633145793e-05, 'epoch': 1.81}
{'loss': 0.7667, 'learning_rate': 2.9826782476114073e-05, 'epoch': 1.92}
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
***** Running Evaluation *****
  Num examples = 5000
  Batch size = 16
{'loss': 0.7495, 'learning_rate': 2.9784389676406646e-05, 'epoch': 2.03}
{'loss': 0.7543, 'learning_rate': 2.973739810199627e-05, 'epoch': 2.13}
Saving model checkpoint to ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-4000
Configuration saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-4000/config.json
Model weights saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-4000/pytorch_model.bin
tokenizer config file saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-4000/tokenizer_config.json
Special tokens file saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-4000/special_tokens_map.json
Deleting older checkpoint [../ckpts/bert_adapter_adapter_linear_2022/checkpoint-3000] due to args.save_total_limit
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
***** Running Evaluation *****
  Num examples = 5000
  Batch size = 16
{'eval_loss': 0.49486348032951355, 'eval_f1': 0.03666651976554581, 'eval_runtime': 8.525, 'eval_samples_per_second': 586.509, 'eval_steps_per_second': 36.715, 'epoch': 2.13}
{'loss': 0.7713, 'learning_rate': 2.968582236989917e-05, 'epoch': 2.24}
{'loss': 0.7507, 'learning_rate': 2.9629678523061803e-05, 'epoch': 2.35}
{'loss': 0.7345, 'learning_rate': 2.9568984025370616e-05, 'epoch': 2.45}
{'loss': 0.7292, 'learning_rate': 2.9503757756219807e-05, 'epoch': 2.56}
{'loss': 0.7387, 'learning_rate': 2.9434020004638757e-05, 'epoch': 2.67}
Saving model checkpoint to ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-5000
Configuration saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-5000/config.json
Model weights saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-5000/pytorch_model.bin
tokenizer config file saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-5000/tokenizer_config.json
Special tokens file saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-5000/special_tokens_map.json
{'eval_loss': 0.48766377568244934, 'eval_f1': 0.025364274150026983, 'eval_runtime': 8.2236, 'eval_samples_per_second': 608.003, 'eval_steps_per_second': 38.061, 'epoch': 2.67}
{'loss': 0.7219, 'learning_rate': 2.9359792462981007e-05, 'epoch': 2.77}
{'loss': 0.7318, 'learning_rate': 2.9281098220176736e-05, 'epoch': 2.88}
{'loss': 0.7239, 'learning_rate': 2.9197961754550774e-05, 'epoch': 2.99}
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
***** Running Evaluation *****
  Num examples = 5000
  Batch size = 16
{'loss': 0.7056, 'learning_rate': 2.9110408926208512e-05, 'epoch': 3.09}
{'loss': 0.7097, 'learning_rate': 2.9018466968991913e-05, 'epoch': 3.2}
Saving model checkpoint to ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-6000
Configuration saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-6000/config.json
Model weights saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-6000/pytorch_model.bin
tokenizer config file saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-6000/tokenizer_config.json
Special tokens file saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-6000/special_tokens_map.json
Deleting older checkpoint [../ckpts/bert_adapter_adapter_linear_2022/checkpoint-4000] due to args.save_total_limit
Deleting older checkpoint [../ckpts/bert_adapter_adapter_linear_2022/checkpoint-5000] due to args.save_total_limit
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
***** Running Evaluation *****
  Num examples = 5000
  Batch size = 16
{'eval_loss': 0.46080389618873596, 'eval_f1': 0.0433042859683443, 'eval_runtime': 8.1957, 'eval_samples_per_second': 610.075, 'eval_steps_per_second': 38.191, 'epoch': 3.2}
{'loss': 0.7122, 'learning_rate': 2.8922164482008307e-05, 'epoch': 3.31}
{'loss': 0.6972, 'learning_rate': 2.8821531420734418e-05, 'epoch': 3.41}
{'loss': 0.6978, 'learning_rate': 2.8716599087698565e-05, 'epoch': 3.52}
{'loss': 0.678, 'learning_rate': 2.8607400122743805e-05, 'epoch': 3.63}
{'loss': 0.6972, 'learning_rate': 2.8493968492875104e-05, 'epoch': 3.73}
Saving model checkpoint to ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-7000
Configuration saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-7000/config.json
Model weights saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-7000/pytorch_model.bin
tokenizer config file saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-7000/tokenizer_config.json
Special tokens file saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-7000/special_tokens_map.json
Deleting older checkpoint [../ckpts/bert_adapter_adapter_linear_2022/checkpoint-6000] due to args.save_total_limit
{'eval_loss': 0.4544120728969574, 'eval_f1': 0.06858867185907108, 'eval_runtime': 8.2464, 'eval_samples_per_second': 606.322, 'eval_steps_per_second': 37.956, 'epoch': 3.73}
{'loss': 0.6957, 'learning_rate': 2.837633948169371e-05, 'epoch': 3.84}
{'loss': 0.6876, 'learning_rate': 2.825454967842195e-05, 'epoch': 3.95}
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
***** Running Evaluation *****
  Num examples = 5000
  Batch size = 16
{'loss': 0.6935, 'learning_rate': 2.8128636966521956e-05, 'epoch': 4.05}
{'loss': 0.6727, 'learning_rate': 2.7998640511911757e-05, 'epoch': 4.16}
{'loss': 0.6877, 'learning_rate': 2.7864600750782507e-05, 'epoch': 4.27}
Saving model checkpoint to ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-8000
Configuration saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-8000/config.json
Model weights saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-8000/pytorch_model.bin
tokenizer config file saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-8000/tokenizer_config.json
Special tokens file saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-8000/special_tokens_map.json
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
***** Running Evaluation *****
  Num examples = 5000
  Batch size = 16
{'eval_loss': 0.44382333755493164, 'eval_f1': 0.06423682691514802, 'eval_runtime': 8.1946, 'eval_samples_per_second': 610.154, 'eval_steps_per_second': 38.196, 'epoch': 4.27}
{'loss': 0.6706, 'learning_rate': 2.7726559377020532e-05, 'epoch': 4.37}
{'loss': 0.6722, 'learning_rate': 2.7584559329238218e-05, 'epoch': 4.48}
{'loss': 0.6797, 'learning_rate': 2.7438644777417673e-05, 'epoch': 4.59}
{'loss': 0.6738, 'learning_rate': 2.728886110917141e-05, 'epoch': 4.69}
{'loss': 0.6802, 'learning_rate': 2.7135254915624213e-05, 'epoch': 4.8}
Saving model checkpoint to ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-9000
Configuration saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-9000/config.json
Model weights saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-9000/pytorch_model.bin
tokenizer config file saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-9000/tokenizer_config.json
Special tokens file saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-9000/special_tokens_map.json
Deleting older checkpoint [../ckpts/bert_adapter_adapter_linear_2022/checkpoint-7000] due to args.save_total_limit
Deleting older checkpoint [../ckpts/bert_adapter_adapter_linear_2022/checkpoint-8000] due to args.save_total_limit
{'eval_loss': 0.44511425495147705, 'eval_f1': 0.06915673149285063, 'eval_runtime': 8.2912, 'eval_samples_per_second': 603.051, 'eval_steps_per_second': 37.751, 'epoch': 4.8}
{'loss': 0.6618, 'learning_rate': 2.697787397692072e-05, 'epoch': 4.91}
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
***** Running Evaluation *****
  Num examples = 5000
  Batch size = 16
{'loss': 0.6718, 'learning_rate': 2.6816767247363108e-05, 'epoch': 5.01}
{'loss': 0.6599, 'learning_rate': 2.6651984840183545e-05, 'epoch': 5.12}
{'loss': 0.6494, 'learning_rate': 2.6483578011956173e-05, 'epoch': 5.23}
{'loss': 0.6505, 'learning_rate': 2.6311599146653446e-05, 'epoch': 5.33}
Saving model checkpoint to ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-10000
Configuration saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-10000/config.json
Model weights saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-10000/pytorch_model.bin
tokenizer config file saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-10000/tokenizer_config.json
Special tokens file saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-10000/special_tokens_map.json
Deleting older checkpoint [../ckpts/bert_adapter_adapter_linear_2022/checkpoint-9000] due to args.save_total_limit
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
***** Running Evaluation *****
  Num examples = 5000
  Batch size = 16
{'eval_loss': 0.4357234835624695, 'eval_f1': 0.07148037714562011, 'eval_runtime': 8.3098, 'eval_samples_per_second': 601.697, 'eval_steps_per_second': 37.666, 'epoch': 5.33}
{'loss': 0.6654, 'learning_rate': 2.613610173935175e-05, 'epoch': 5.44}
{'loss': 0.653, 'learning_rate': 2.5957140379591465e-05, 'epoch': 5.55}
{'loss': 0.6541, 'learning_rate': 2.5774770734396533e-05, 'epoch': 5.65}
{'loss': 0.6615, 'learning_rate': 2.5589049530958885e-05, 'epoch': 5.76}
{'loss': 0.6314, 'learning_rate': 2.5400034538993133e-05, 'epoch': 5.87}
Saving model checkpoint to ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-11000
Configuration saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-11000/config.json
Model weights saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-11000/pytorch_model.bin
tokenizer config file saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-11000/tokenizer_config.json
Special tokens file saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-11000/special_tokens_map.json
Deleting older checkpoint [../ckpts/bert_adapter_adapter_linear_2022/checkpoint-10000] due to args.save_total_limit
{'eval_loss': 0.4365357458591461, 'eval_f1': 0.08293942403177755, 'eval_runtime': 8.2032, 'eval_samples_per_second': 609.522, 'eval_steps_per_second': 38.156, 'epoch': 5.87}
{'loss': 0.6571, 'learning_rate': 2.5207784552766914e-05, 'epoch': 5.97}
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
***** Running Evaluation *****
  Num examples = 5000
  Batch size = 16
{'loss': 0.6587, 'learning_rate': 2.501235937281259e-05, 'epoch': 6.08}
{'loss': 0.6417, 'learning_rate': 2.4813819787325925e-05, 'epoch': 6.19}
{'loss': 0.6545, 'learning_rate': 2.4612227553257545e-05, 'epoch': 6.29}
{'loss': 0.6456, 'learning_rate': 2.4407645377103056e-05, 'epoch': 6.4}
Saving model checkpoint to ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-12000
Configuration saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-12000/config.json
Model weights saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-12000/pytorch_model.bin
tokenizer config file saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-12000/tokenizer_config.json
Special tokens file saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-12000/special_tokens_map.json
Deleting older checkpoint [../ckpts/bert_adapter_adapter_linear_2022/checkpoint-11000] due to args.save_total_limit
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
***** Running Evaluation *****
  Num examples = 5000
  Batch size = 16
{'eval_loss': 0.43134990334510803, 'eval_f1': 0.08715844879634646, 'eval_runtime': 8.2747, 'eval_samples_per_second': 604.251, 'eval_steps_per_second': 37.826, 'epoch': 6.4}
{'loss': 0.64, 'learning_rate': 2.4200136895397818e-05, 'epoch': 6.51}
{'loss': 0.6359, 'learning_rate': 2.3989766654922404e-05, 'epoch': 6.61}
{'loss': 0.6407, 'learning_rate': 2.3776600092624925e-05, 'epoch': 6.72}
{'loss': 0.6428, 'learning_rate': 2.356070351526648e-05, 'epoch': 6.83}
{'loss': 0.641, 'learning_rate': 2.3342144078796007e-05, 'epoch': 6.93}
Saving model checkpoint to ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-13000
Configuration saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-13000/config.json
Model weights saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-13000/pytorch_model.bin
tokenizer config file saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-13000/tokenizer_config.json
Special tokens file saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-13000/special_tokens_map.json
{'eval_loss': 0.4439344108104706, 'eval_f1': 0.08488843008020329, 'eval_runtime': 8.3035, 'eval_samples_per_second': 602.158, 'eval_steps_per_second': 37.695, 'epoch': 6.93}
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
***** Running Evaluation *****
  Num examples = 5000
  Batch size = 16
{'loss': 0.6459, 'learning_rate': 2.312098976746107e-05, 'epoch': 7.04}
{'loss': 0.6368, 'learning_rate': 2.2897309372660873e-05, 'epoch': 7.15}
{'loss': 0.6296, 'learning_rate': 2.267117247154833e-05, 'epoch': 7.25}
{'loss': 0.6385, 'learning_rate': 2.2442649405387632e-05, 'epoch': 7.36}
{'loss': 0.6307, 'learning_rate': 2.2211811257674192e-05, 'epoch': 7.47}
Saving model checkpoint to ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-14000
Configuration saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-14000/config.json
Model weights saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-14000/pytorch_model.bin
tokenizer config file saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-14000/tokenizer_config.json
Special tokens file saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-14000/special_tokens_map.json
Deleting older checkpoint [../ckpts/bert_adapter_adapter_linear_2022/checkpoint-12000] due to args.save_total_limit
Deleting older checkpoint [../ckpts/bert_adapter_adapter_linear_2022/checkpoint-13000] due to args.save_total_limit
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
***** Running Evaluation *****
  Num examples = 5000
  Batch size = 16
{'eval_loss': 0.433413565158844, 'eval_f1': 0.08779576587795766, 'eval_runtime': 8.3469, 'eval_samples_per_second': 599.025, 'eval_steps_per_second': 37.499, 'epoch': 7.47}
{'loss': 0.6284, 'learning_rate': 2.1978729832023682e-05, 'epoch': 7.57}
{'loss': 0.6312, 'learning_rate': 2.1743477629837057e-05, 'epoch': 7.68}
{'loss': 0.6424, 'learning_rate': 2.15061278277486e-05, 'epoch': 7.79}
{'loss': 0.6143, 'learning_rate': 2.126675425486383e-05, 'epoch': 7.89}
{'loss': 0.6202, 'learning_rate': 2.1025431369794546e-05, 'epoch': 8.0}
Saving model checkpoint to ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-15000
Configuration saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-15000/config.json
Model weights saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-15000/pytorch_model.bin
tokenizer config file saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-15000/tokenizer_config.json
Special tokens file saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-15000/special_tokens_map.json
Deleting older checkpoint [../ckpts/bert_adapter_adapter_linear_2022/checkpoint-14000] due to args.save_total_limit
{'eval_loss': 0.42297881841659546, 'eval_f1': 0.08795199747355124, 'eval_runtime': 8.2507, 'eval_samples_per_second': 606.01, 'eval_steps_per_second': 37.936, 'epoch': 8.0}
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
***** Running Evaluation *****
  Num examples = 5000
  Batch size = 16
{'loss': 0.6296, 'learning_rate': 2.0782234237497998e-05, 'epoch': 8.11}
{'loss': 0.6261, 'learning_rate': 2.053723850592749e-05, 'epoch': 8.21}
{'loss': 0.6208, 'learning_rate': 2.029052038250162e-05, 'epoch': 8.32}
{'loss': 0.6305, 'learning_rate': 2.004215661039947e-05, 'epoch': 8.43}
{'loss': 0.6173, 'learning_rate': 1.9792224444689223e-05, 'epoch': 8.53}
Saving model checkpoint to ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-16000
Configuration saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-16000/config.json
Model weights saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-16000/pytorch_model.bin
tokenizer config file saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-16000/tokenizer_config.json
Special tokens file saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-16000/special_tokens_map.json
Deleting older checkpoint [../ckpts/bert_adapter_adapter_linear_2022/checkpoint-15000] due to args.save_total_limit
{'eval_loss': 0.41802895069122314, 'eval_f1': 0.08929958068022985, 'eval_runtime': 8.3142, 'eval_samples_per_second': 601.379, 'eval_steps_per_second': 37.646, 'epoch': 8.53}
{'loss': 0.6148, 'learning_rate': 1.954080162829745e-05, 'epoch': 8.64}
{'loss': 0.6283, 'learning_rate': 1.9287966367826726e-05, 'epoch': 8.75}
{'loss': 0.6143, 'learning_rate': 1.9033797309228984e-05, 'epoch': 8.85}
{'loss': 0.6125, 'learning_rate': 1.8778373513342223e-05, 'epoch': 8.96}
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
***** Running Evaluation *****
  Num examples = 5000
  Batch size = 16
{'loss': 0.6269, 'learning_rate': 1.8521774431298116e-05, 'epoch': 9.07}
Saving model checkpoint to ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-17000
Configuration saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-17000/config.json
Model weights saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-17000/pytorch_model.bin
tokenizer config file saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-17000/tokenizer_config.json
Special tokens file saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-17000/special_tokens_map.json
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
***** Running Evaluation *****
  Num examples = 5000
  Batch size = 16
{'eval_loss': 0.41822177171707153, 'eval_f1': 0.0817394805717737, 'eval_runtime': 8.3059, 'eval_samples_per_second': 601.981, 'eval_steps_per_second': 37.684, 'epoch': 9.07}
{'loss': 0.6031, 'learning_rate': 1.826407987980829e-05, 'epoch': 9.17}
{'loss': 0.6148, 'learning_rate': 1.800537001633682e-05, 'epoch': 9.28}
{'loss': 0.5986, 'learning_rate': 1.774572531416679e-05, 'epoch': 9.39}
{'loss': 0.6301, 'learning_rate': 1.7485226537368565e-05, 'epoch': 9.49}
{'loss': 0.6099, 'learning_rate': 1.722395471567763e-05, 'epoch': 9.6}
Saving model checkpoint to ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-18000
Configuration saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-18000/config.json
Model weights saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-18000/pytorch_model.bin
tokenizer config file saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-18000/tokenizer_config.json
Special tokens file saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-18000/special_tokens_map.json
Deleting older checkpoint [../ckpts/bert_adapter_adapter_linear_2022/checkpoint-17000] due to args.save_total_limit
{'eval_loss': 0.42782166600227356, 'eval_f1': 0.08875079668578713, 'eval_runtime': 8.3129, 'eval_samples_per_second': 601.472, 'eval_steps_per_second': 37.652, 'epoch': 9.6}
{'loss': 0.6285, 'learning_rate': 1.6961991119289818e-05, 'epoch': 9.71}
{'loss': 0.61, 'learning_rate': 1.669941723358169e-05, 'epoch': 9.81}
{'loss': 0.618, 'learning_rate': 1.643631473376405e-05, 'epoch': 9.92}
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
***** Running Evaluation *****
  Num examples = 5000
  Batch size = 16
{'loss': 0.612, 'learning_rate': 1.6172765459476335e-05, 'epoch': 10.03}
{'loss': 0.6153, 'learning_rate': 1.5908851389329907e-05, 'epoch': 10.13}
Saving model checkpoint to ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-19000
Configuration saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-19000/config.json
Model weights saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-19000/pytorch_model.bin
tokenizer config file saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-19000/tokenizer_config.json
Special tokens file saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-19000/special_tokens_map.json
Deleting older checkpoint [../ckpts/bert_adapter_adapter_linear_2022/checkpoint-16000] due to args.save_total_limit
Deleting older checkpoint [../ckpts/bert_adapter_adapter_linear_2022/checkpoint-18000] due to args.save_total_limit
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
***** Running Evaluation *****
  Num examples = 5000
  Batch size = 16
{'eval_loss': 0.41678398847579956, 'eval_f1': 0.09193238420206298, 'eval_runtime': 8.2944, 'eval_samples_per_second': 602.82, 'eval_steps_per_second': 37.737, 'epoch': 10.13}
{'loss': 0.6063, 'learning_rate': 1.564465461540811e-05, 'epoch': 10.24}
{'loss': 0.6058, 'learning_rate': 1.5380257317731035e-05, 'epoch': 10.35}
{'loss': 0.616, 'learning_rate': 1.5115741738692894e-05, 'epoch': 10.45}
{'loss': 0.6131, 'learning_rate': 1.4851190157480054e-05, 'epoch': 10.56}
{'loss': 0.5981, 'learning_rate': 1.4586684864477572e-05, 'epoch': 10.67}
Saving model checkpoint to ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-20000
Configuration saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-20000/config.json
Model weights saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-20000/pytorch_model.bin
tokenizer config file saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-20000/tokenizer_config.json
Special tokens file saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-20000/special_tokens_map.json
Deleting older checkpoint [../ckpts/bert_adapter_adapter_linear_2022/checkpoint-19000] due to args.save_total_limit
{'eval_loss': 0.40920212864875793, 'eval_f1': 0.09420755930536114, 'eval_runtime': 8.2817, 'eval_samples_per_second': 603.744, 'eval_steps_per_second': 37.794, 'epoch': 10.67}
{'loss': 0.6078, 'learning_rate': 1.4322308135672268e-05, 'epoch': 10.77}
{'loss': 0.6133, 'learning_rate': 1.40581422070603e-05, 'epoch': 10.88}
{'loss': 0.6139, 'learning_rate': 1.3794269249067122e-05, 'epoch': 10.99}
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
***** Running Evaluation *****
  Num examples = 5000
  Batch size = 16
{'loss': 0.5971, 'learning_rate': 1.3530771340987894e-05, 'epoch': 11.09}
{'loss': 0.602, 'learning_rate': 1.3267730445456208e-05, 'epoch': 11.2}
Saving model checkpoint to ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-21000
Configuration saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-21000/config.json
Model weights saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-21000/pytorch_model.bin
tokenizer config file saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-21000/tokenizer_config.json
Special tokens file saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-21000/special_tokens_map.json
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
***** Running Evaluation *****
  Num examples = 5000
  Batch size = 16
{'eval_loss': 0.41238829493522644, 'eval_f1': 0.09321135498056461, 'eval_runtime': 8.4106, 'eval_samples_per_second': 594.489, 'eval_steps_per_second': 37.215, 'epoch': 11.2}
{'loss': 0.6154, 'learning_rate': 1.300522838294912e-05, 'epoch': 11.31}
{'loss': 0.5998, 'learning_rate': 1.2743346806336373e-05, 'epoch': 11.41}
{'loss': 0.6074, 'learning_rate': 1.2482167175481786e-05, 'epoch': 11.52}
{'loss': 0.6057, 'learning_rate': 1.2221770731904661e-05, 'epoch': 11.63}
{'loss': 0.5979, 'learning_rate': 1.1962238473509122e-05, 'epoch': 11.73}
Saving model checkpoint to ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-22000
Configuration saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-22000/config.json
Model weights saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-22000/pytorch_model.bin
tokenizer config file saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-22000/tokenizer_config.json
Special tokens file saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-22000/special_tokens_map.json
Deleting older checkpoint [../ckpts/bert_adapter_adapter_linear_2022/checkpoint-20000] due to args.save_total_limit
Deleting older checkpoint [../ckpts/bert_adapter_adapter_linear_2022/checkpoint-21000] due to args.save_total_limit
{'eval_loss': 0.4102969169616699, 'eval_f1': 0.10099188458070334, 'eval_runtime': 8.289, 'eval_samples_per_second': 603.207, 'eval_steps_per_second': 37.761, 'epoch': 11.73}
{'loss': 0.608, 'learning_rate': 1.1703651129389203e-05, 'epoch': 11.84}
{'loss': 0.6, 'learning_rate': 1.1446089134717594e-05, 'epoch': 11.95}
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
***** Running Evaluation *****
  Num examples = 5000
  Batch size = 16
{'loss': 0.5927, 'learning_rate': 1.1189632605725762e-05, 'epoch': 12.05}
{'loss': 0.6116, 'learning_rate': 1.0934361314783339e-05, 'epoch': 12.16}
{'loss': 0.6023, 'learning_rate': 1.0680354665584412e-05, 'epoch': 12.27}
Saving model checkpoint to ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-23000
Configuration saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-23000/config.json
Model weights saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-23000/pytorch_model.bin
tokenizer config file saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-23000/tokenizer_config.json
Special tokens file saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-23000/special_tokens_map.json
Deleting older checkpoint [../ckpts/bert_adapter_adapter_linear_2022/checkpoint-22000] due to args.save_total_limit
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
***** Running Evaluation *****
  Num examples = 5000
  Batch size = 16
{'eval_loss': 0.40933510661125183, 'eval_f1': 0.10207755193879847, 'eval_runtime': 8.2839, 'eval_samples_per_second': 603.577, 'eval_steps_per_second': 37.784, 'epoch': 12.27}
{'loss': 0.5954, 'learning_rate': 1.0427691668448533e-05, 'epoch': 12.37}
{'loss': 0.5957, 'learning_rate': 1.0176450915744072e-05, 'epoch': 12.48}
{'loss': 0.6058, 'learning_rate': 9.926710557441581e-06, 'epoch': 12.59}
{'loss': 0.5844, 'learning_rate': 9.67854827680478e-06, 'epoch': 12.69}
{'loss': 0.5959, 'learning_rate': 9.432041266226686e-06, 'epoch': 12.8}
Saving model checkpoint to ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-24000
Configuration saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-24000/config.json
Model weights saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-24000/pytorch_model.bin
tokenizer config file saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-24000/tokenizer_config.json
Special tokens file saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-24000/special_tokens_map.json
{'eval_loss': 0.42148253321647644, 'eval_f1': 0.09654796228473467, 'eval_runtime': 8.3234, 'eval_samples_per_second': 600.719, 'eval_steps_per_second': 37.605, 'epoch': 12.8}
{'loss': 0.5956, 'learning_rate': 9.187266203218457e-06, 'epoch': 12.91}
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
***** Running Evaluation *****
  Num examples = 5000
  Batch size = 16
{'loss': 0.5996, 'learning_rate': 8.94429922655837e-06, 'epoch': 13.01}
{'loss': 0.6048, 'learning_rate': 8.703215912608416e-06, 'epoch': 13.12}
{'loss': 0.6067, 'learning_rate': 8.46409125180579e-06, 'epoch': 13.23}
{'loss': 0.5984, 'learning_rate': 8.226999625336663e-06, 'epoch': 13.33}
Saving model checkpoint to ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-25000
Configuration saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-25000/config.json
Model weights saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-25000/pytorch_model.bin
tokenizer config file saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-25000/tokenizer_config.json
Special tokens file saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-25000/special_tokens_map.json
Deleting older checkpoint [../ckpts/bert_adapter_adapter_linear_2022/checkpoint-24000] due to args.save_total_limit
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
***** Running Evaluation *****
  Num examples = 5000
  Batch size = 16
{'eval_loss': 0.4152083992958069, 'eval_f1': 0.09697158670148498, 'eval_runtime': 8.3102, 'eval_samples_per_second': 601.669, 'eval_steps_per_second': 37.664, 'epoch': 13.33}
{'loss': 0.5842, 'learning_rate': 7.992014781999454e-06, 'epoch': 13.44}
{'loss': 0.5936, 'learning_rate': 7.75920981526484e-06, 'epoch': 13.55}
{'loss': 0.5928, 'learning_rate': 7.528657140539548e-06, 'epoch': 13.65}
{'loss': 0.597, 'learning_rate': 7.3004284726411315e-06, 'epoch': 13.76}
{'loss': 0.6006, 'learning_rate': 7.074594803490618e-06, 'epoch': 13.87}
Saving model checkpoint to ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-26000
Configuration saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-26000/config.json
Model weights saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-26000/pytorch_model.bin
tokenizer config file saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-26000/tokenizer_config.json
Special tokens file saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-26000/special_tokens_map.json
Deleting older checkpoint [../ckpts/bert_adapter_adapter_linear_2022/checkpoint-23000] due to args.save_total_limit
Deleting older checkpoint [../ckpts/bert_adapter_adapter_linear_2022/checkpoint-25000] due to args.save_total_limit
{'eval_loss': 0.4087716042995453, 'eval_f1': 0.10477158889767635, 'eval_runtime': 8.2916, 'eval_samples_per_second': 603.017, 'eval_steps_per_second': 37.749, 'epoch': 13.87}
{'loss': 0.5874, 'learning_rate': 6.851226380030057e-06, 'epoch': 13.97}
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
***** Running Evaluation *****
  Num examples = 5000
  Batch size = 16
{'loss': 0.6065, 'learning_rate': 6.630392682371761e-06, 'epoch': 14.08}
{'loss': 0.5935, 'learning_rate': 6.412162402186106e-06, 'epoch': 14.19}
{'loss': 0.5994, 'learning_rate': 6.196603421334558e-06, 'epoch': 14.29}
{'loss': 0.5963, 'learning_rate': 5.983782790754624e-06, 'epoch': 14.4}
Saving model checkpoint to ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-27000
Configuration saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-27000/config.json
Model weights saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-27000/pytorch_model.bin
tokenizer config file saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-27000/tokenizer_config.json
Special tokens file saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-27000/special_tokens_map.json
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
***** Running Evaluation *****
  Num examples = 5000
  Batch size = 16
{'eval_loss': 0.4056284725666046, 'eval_f1': 0.10140780588302621, 'eval_runtime': 8.2977, 'eval_samples_per_second': 602.578, 'eval_steps_per_second': 37.721, 'epoch': 14.4}
{'loss': 0.583, 'learning_rate': 5.773766709603221e-06, 'epoch': 14.51}
{'loss': 0.589, 'learning_rate': 5.566620504665043e-06, 'epoch': 14.61}
{'loss': 0.5852, 'learning_rate': 5.362408610032257e-06, 'epoch': 14.72}
{'loss': 0.5866, 'learning_rate': 5.161194547061893e-06, 'epoch': 14.83}
{'loss': 0.6059, 'learning_rate': 4.963040904617131e-06, 'epoch': 14.93}
Saving model checkpoint to ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-28000
Configuration saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-28000/config.json
Model weights saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-28000/pytorch_model.bin
tokenizer config file saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-28000/tokenizer_config.json
Special tokens file saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-28000/special_tokens_map.json
Deleting older checkpoint [../ckpts/bert_adapter_adapter_linear_2022/checkpoint-27000] due to args.save_total_limit
{'eval_loss': 0.41059771180152893, 'eval_f1': 0.10056387322574373, 'eval_runtime': 8.2447, 'eval_samples_per_second': 606.449, 'eval_steps_per_second': 37.964, 'epoch': 14.93}
/dssg/home/acct-stu/stu915/.conda/envs/py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
***** Running Evaluation *****
  Num examples = 5000
  Batch size = 16
{'loss': 0.5846, 'learning_rate': 4.768009319598653e-06, 'epoch': 15.04}
{'loss': 0.5934, 'learning_rate': 4.576160457772118e-06, 'epoch': 15.15}
{'loss': 0.5965, 'learning_rate': 4.3875539948976964e-06, 'epoch': 15.25}
{'loss': 0.5848, 'learning_rate': 4.202248598167549e-06, 'epoch': 15.36}
{'loss': 0.595, 'learning_rate': 4.020301907957075e-06, 'epoch': 15.47}
Saving model checkpoint to ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-29000
Configuration saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-29000/config.json
Model weights saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-29000/pytorch_model.bin
tokenizer config file saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-29000/tokenizer_config.json
Special tokens file saved in ../ckpts/bert_adapter_adapter_linear_2022/checkpoint-29000/special_tokens_map.json
Deleting older checkpoint [../ckpts/bert_adapter_adapter_linear_2022/checkpoint-28000] due to args.save_total_limit
slurmstepd: error: *** JOB 15208367 ON gpu07 CANCELLED AT 2022-06-09T11:19:43 ***
